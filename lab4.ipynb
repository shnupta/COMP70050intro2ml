{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shnupta/COMP70050intro2ml/blob/main/lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MBmOdXoenFw"
      },
      "source": [
        "# Lab 4: Simple Linear Regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3iS23ADfo4w"
      },
      "source": [
        "## Version history\n",
        "\n",
        "| Date | Author | Description |\n",
        "|:----:|:------:|:------------|\n",
        "2021-02-03 | Josiah Wang | First version | "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EbbkgqOgZK_"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "The aim of this lab exercise is for you to gain some experience implementing and training a simple linear regression model from scratch. This will help you improve your understanding of linear regression and machine learning optimisation. \n",
        "\n",
        "By the end of this lab exercise, you will have \n",
        "- implemented a simple linear regression model \n",
        "- defined and implemented a loss function\n",
        "- optimised the parameters of your model using gradient descent\n",
        "\n",
        "There will be a bit less coding required on your side in this exercise compared to previous exercises. The aim is for you to try to really understand linear regression at implementation level to complement the lectures, which will help you in future weeks as you move on to Neural Networks in your coursework assignments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0iaMjnIWEpe"
      },
      "source": [
        "## Simple Linear Regression\n",
        "\n",
        "In this tutorial, we will focus on the **regression task**. For simplicity, we will implement a *simple linear regression* model with one input variable and one output variable. More specifically, our task is to predict the value of $y$ given the input $x$.\n",
        "\n",
        "Let us develop our simple linear regressor with a simple toy example to make sure that our model works correctly. You can later apply it to a bigger dataset if desired."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w2N6AZsYJUd",
        "outputId": "9f4454dc-0d3d-4182-d8e1-ccfc3bfe65e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# toy dataset\n",
        "x_train = np.array([1.0, 1.2, 2.0, 3.5, 4.0, 5.0])\n",
        "y_train = np.array([3.1, 3.5, 5.0, 7.9, 9.1, 10.9])\n",
        "x_test = np.array([2.5, 3.0, 4.5])\n",
        "y_test = np.array([6.0, 7.0, 10.1])\n",
        "\n",
        "# plot toy data\n",
        "plt.scatter(x_train, y_train, c=\"blue\")\n",
        "plt.scatter(x_test, y_test, c=\"red\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.show()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YagIkbxvaC-b"
      },
      "source": [
        "### Model\n",
        "\n",
        "As you should be able to see from the plot, the toy dataset can almost be perfectly modelled with a straight line. The model should also be able to pretty accurately predict the value of $y$ of the test set.\n",
        "\n",
        "Assuming you still remember your high school Maths, a straight line is represented as $y = wx + b$, where $w$ is the slope and $b$ the intercept/bias. Our objective is to find the line that best fits our training data. More specifically, we want our regressor to automatically learn the parameters $w$ and $b$ such that we can accurately predict the real-valued label ${\\hat y}$ given an example $x$. The objective is to get ${\\hat y}$ to be as close as possible to the values of $y$ of the training data (and presumably the true $y$).\n",
        "\n",
        "Let us now build our simple linear regression model. Complete the `forward()` method of the `SimpleLinearRegression` class below to return the value of the output $y$ given an input `x` and the current weight `w` and bias `b` of the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLNpOSjXUQrY",
        "outputId": "d0eefa1b-6967-4a36-ae93-491e4ac26958",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from numpy.random import default_rng\n",
        "\n",
        "class SimpleLinearRegression:\n",
        "    def __init__(self, random_generator=default_rng()):\n",
        "        # initialise the slope with a random value drawn from a standard normal \n",
        "        # distribution (mean=0, stddev=1)\n",
        "        self.w = random_generator.standard_normal()\n",
        "\n",
        "        # initialise bias to 0 \n",
        "        self.b = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Perform forward pass given an input x\n",
        "\n",
        "        Args:\n",
        "            x (float): input instance\n",
        "\n",
        "        Returns:\n",
        "            float: the output of the model given the current weights\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: Complete this\n",
        "        return self.w * x + self.b\n",
        "\n",
        "    def loss(self, x, y):\n",
        "        \"\"\" Placeholder for later\"\"\"\n",
        "        pass\n",
        "\n",
        "    def gradient(self, x, y):\n",
        "        \"\"\" Placeholder for later\"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "## Quick test: This should return 8\n",
        "model = SimpleLinearRegression()\n",
        "model.w = 3\n",
        "model.b = 2\n",
        "x = 2\n",
        "y_hat = model.forward(x)\n",
        "print(y_hat) # should print 8"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9mtsqFXBNNa"
      },
      "source": [
        "### Loss function\n",
        "\n",
        "From the plot and the numbers, you may have manually worked out that the 'best' line would be $\\hat{y} \\approx 2x+1$, that is, the optimal parameter values are $w \\approx 2$ and $b \\approx 1$.\n",
        "\n",
        "What constitutes the 'best' line? We will first have to define what 'best' actually means. Intuitively, the 'best' line would be the one that goes through all training points as closely as possible.\n",
        "\n",
        "To enable our model to automatically learn what the parameter values of the 'best' line are from training examples, we will have to formally define that we mean by 'best'. We define this via a *loss function* (or cost function). For this tutorial, we will use the loss function as in the lectures - the **sum of squared differences** between the predicted label vs. the ground truth label across the training instances. \n",
        "\n",
        "$$L = \\frac{1}{2} \\sum_{i=1}^{N} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2 = \\frac{1}{2} \\sum_{i=1}^{N} \\left(wx^{(i)} + b - y^{(i)}\\right)^2$$\n",
        "\n",
        "Our objective is to select the parameters $\\theta = \\{ w, b \\}$ that **minimise** the loss (or error).\n",
        "\n",
        "$$\\theta = argmin_{\\theta} \\, L$$\n",
        "\n",
        "To make things easy, let us implement the loss function for a **single** instance $x$:\n",
        "\n",
        "$$L^{(i)} = \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2$$\n",
        "\n",
        "Complete the `loss()` method of `SimpleLinearRegression` below to return the individual loss for an instance `x`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZM0MAgiE2UZ",
        "outputId": "ab0c1a10-eac8-4f7b-9e90-60973ab4fca5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Loss method for SimpleLinearRegression\n",
        "def loss(self, x, y):\n",
        "    \"\"\" Compute the loss for an input x\n",
        "\n",
        "    Args:\n",
        "        x (float): input instance\n",
        "        y (float): ground truth output\n",
        "\n",
        "    Returns:\n",
        "        float: the model loss for an instance x\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Complete this\n",
        "\n",
        "    return np.square(self.forward(x) - y)\n",
        "\n",
        "\n",
        "# A quick hack to bind this function as the SimpleLinearRegression.loss() method\n",
        "SimpleLinearRegression.loss = loss\n",
        "\n",
        "\n",
        "## Quick test: This should return 0.25\n",
        "model = SimpleLinearRegression()\n",
        "model.w = 3\n",
        "model.b = 2\n",
        "x = 2.0\n",
        "y = 8.5\n",
        "test_loss = model.loss(x, y)\n",
        "print(test_loss) # should print 0.25"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxwg9EPiGu2l"
      },
      "source": [
        "### Optimisation by brute force search\n",
        "\n",
        "Now, how do we get the model to automatically figure out the optimal parameters from training data? Remember that the optimal parameter values are the ones that minimise the loss function. A naive approach would be to compute the loss for different combinations of $w$ and $b$ and selecting the combination that results in the smallest loss.\n",
        "\n",
        "The code below will search for $w$ between $0$ and $4$, and for $b$ between $0$ and $2$ to find the best combination of $w$ and $b$. Examine the code, and try to understand what it is doing, then run the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyV8OCZSIdrj",
        "outputId": "8bdeeed5-6790-4a15-e6d3-f4834585352f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = SimpleLinearRegression()\n",
        "\n",
        "# to store all losses for later use\n",
        "losses = []\n",
        "\n",
        "# the parameters to search\n",
        "weights = np.arange(0, 4.1, 0.2) \n",
        "biases = np.arange(0, 2.1, 0.2)\n",
        "\n",
        "# for storing the loss in a matrix for visualisation later\n",
        "loss_matrix = np.zeros((len(weights), len(biases)))\n",
        "\n",
        "# compute loss for each (w,b) combination\n",
        "for i, w in enumerate(weights):\n",
        "    for j, b in enumerate(biases):\n",
        "        print(f\"(w={w:.1f}, b={b:.1f})\")\n",
        "        \n",
        "        # setup weights of model\n",
        "        model.w = w\n",
        "        model.b = b\n",
        "\n",
        "        sum_loss = 0\n",
        "        # for each example\n",
        "        for (x, y) in zip(x_train, y_train):\n",
        "            # compute the loss for this example\n",
        "            single_loss = model.loss(x, y)\n",
        "\n",
        "            # and add it to the sum\n",
        "            sum_loss += single_loss\n",
        "\n",
        "            # print out the values just to make sure everything is working correctly\n",
        "            y_hat = model.forward(x)\n",
        "            print(f\"    x: {x}, y: {y}, y_hat: {y_hat:.1f}, loss: {single_loss:.2f}\")\n",
        "\n",
        "        # print out the sum of individual losses\n",
        "        # I multiplied by 0.5 to be consistent with the equation earlier, \n",
        "        # but this is not necessary in practice as this is a constant\n",
        "        print(f\"    Loss = {(0.5 * sum_loss):.4f}\\n\")\n",
        "\n",
        "        # store the losses and the corresponding (w,b) for later use\n",
        "        losses.append((0.5*sum_loss, w, b))\n",
        "\n",
        "        # store the losses in a matrix form for visualisation later\n",
        "        loss_matrix[i,j] = 0.5 * sum_loss\n",
        "\n",
        "# find combination with minimum loss\n",
        "(min_loss, best_w, best_b) = min(losses, key=lambda x:x[0])\n",
        "print(\"BEST:\")\n",
        "print(f\"w={best_w}, b={best_b}, loss={min_loss:.4f}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(w=0.0, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.0, loss: 9.61\n",
            "    x: 1.2, y: 3.5, y_hat: 0.0, loss: 12.25\n",
            "    x: 2.0, y: 5.0, y_hat: 0.0, loss: 25.00\n",
            "    x: 3.5, y: 7.9, y_hat: 0.0, loss: 62.41\n",
            "    x: 4.0, y: 9.1, y_hat: 0.0, loss: 82.81\n",
            "    x: 5.0, y: 10.9, y_hat: 0.0, loss: 118.81\n",
            "    Loss = 155.4450\n",
            "\n",
            "(w=0.0, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.2, loss: 8.41\n",
            "    x: 1.2, y: 3.5, y_hat: 0.2, loss: 10.89\n",
            "    x: 2.0, y: 5.0, y_hat: 0.2, loss: 23.04\n",
            "    x: 3.5, y: 7.9, y_hat: 0.2, loss: 59.29\n",
            "    x: 4.0, y: 9.1, y_hat: 0.2, loss: 79.21\n",
            "    x: 5.0, y: 10.9, y_hat: 0.2, loss: 114.49\n",
            "    Loss = 147.6650\n",
            "\n",
            "(w=0.0, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.4, loss: 7.29\n",
            "    x: 1.2, y: 3.5, y_hat: 0.4, loss: 9.61\n",
            "    x: 2.0, y: 5.0, y_hat: 0.4, loss: 21.16\n",
            "    x: 3.5, y: 7.9, y_hat: 0.4, loss: 56.25\n",
            "    x: 4.0, y: 9.1, y_hat: 0.4, loss: 75.69\n",
            "    x: 5.0, y: 10.9, y_hat: 0.4, loss: 110.25\n",
            "    Loss = 140.1250\n",
            "\n",
            "(w=0.0, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.6, loss: 6.25\n",
            "    x: 1.2, y: 3.5, y_hat: 0.6, loss: 8.41\n",
            "    x: 2.0, y: 5.0, y_hat: 0.6, loss: 19.36\n",
            "    x: 3.5, y: 7.9, y_hat: 0.6, loss: 53.29\n",
            "    x: 4.0, y: 9.1, y_hat: 0.6, loss: 72.25\n",
            "    x: 5.0, y: 10.9, y_hat: 0.6, loss: 106.09\n",
            "    Loss = 132.8250\n",
            "\n",
            "(w=0.0, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.8, loss: 5.29\n",
            "    x: 1.2, y: 3.5, y_hat: 0.8, loss: 7.29\n",
            "    x: 2.0, y: 5.0, y_hat: 0.8, loss: 17.64\n",
            "    x: 3.5, y: 7.9, y_hat: 0.8, loss: 50.41\n",
            "    x: 4.0, y: 9.1, y_hat: 0.8, loss: 68.89\n",
            "    x: 5.0, y: 10.9, y_hat: 0.8, loss: 102.01\n",
            "    Loss = 125.7650\n",
            "\n",
            "(w=0.0, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.0, loss: 4.41\n",
            "    x: 1.2, y: 3.5, y_hat: 1.0, loss: 6.25\n",
            "    x: 2.0, y: 5.0, y_hat: 1.0, loss: 16.00\n",
            "    x: 3.5, y: 7.9, y_hat: 1.0, loss: 47.61\n",
            "    x: 4.0, y: 9.1, y_hat: 1.0, loss: 65.61\n",
            "    x: 5.0, y: 10.9, y_hat: 1.0, loss: 98.01\n",
            "    Loss = 118.9450\n",
            "\n",
            "(w=0.0, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.2, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 1.2, loss: 5.29\n",
            "    x: 2.0, y: 5.0, y_hat: 1.2, loss: 14.44\n",
            "    x: 3.5, y: 7.9, y_hat: 1.2, loss: 44.89\n",
            "    x: 4.0, y: 9.1, y_hat: 1.2, loss: 62.41\n",
            "    x: 5.0, y: 10.9, y_hat: 1.2, loss: 94.09\n",
            "    Loss = 112.3650\n",
            "\n",
            "(w=0.0, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 1.4, loss: 4.41\n",
            "    x: 2.0, y: 5.0, y_hat: 1.4, loss: 12.96\n",
            "    x: 3.5, y: 7.9, y_hat: 1.4, loss: 42.25\n",
            "    x: 4.0, y: 9.1, y_hat: 1.4, loss: 59.29\n",
            "    x: 5.0, y: 10.9, y_hat: 1.4, loss: 90.25\n",
            "    Loss = 106.0250\n",
            "\n",
            "(w=0.0, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 1.6, loss: 3.61\n",
            "    x: 2.0, y: 5.0, y_hat: 1.6, loss: 11.56\n",
            "    x: 3.5, y: 7.9, y_hat: 1.6, loss: 39.69\n",
            "    x: 4.0, y: 9.1, y_hat: 1.6, loss: 56.25\n",
            "    x: 5.0, y: 10.9, y_hat: 1.6, loss: 86.49\n",
            "    Loss = 99.9250\n",
            "\n",
            "(w=0.0, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 1.8, loss: 2.89\n",
            "    x: 2.0, y: 5.0, y_hat: 1.8, loss: 10.24\n",
            "    x: 3.5, y: 7.9, y_hat: 1.8, loss: 37.21\n",
            "    x: 4.0, y: 9.1, y_hat: 1.8, loss: 53.29\n",
            "    x: 5.0, y: 10.9, y_hat: 1.8, loss: 82.81\n",
            "    Loss = 94.0650\n",
            "\n",
            "(w=0.0, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 2.0, loss: 2.25\n",
            "    x: 2.0, y: 5.0, y_hat: 2.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 2.0, loss: 34.81\n",
            "    x: 4.0, y: 9.1, y_hat: 2.0, loss: 50.41\n",
            "    x: 5.0, y: 10.9, y_hat: 2.0, loss: 79.21\n",
            "    Loss = 88.4450\n",
            "\n",
            "(w=0.2, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.2, loss: 8.41\n",
            "    x: 1.2, y: 3.5, y_hat: 0.2, loss: 10.63\n",
            "    x: 2.0, y: 5.0, y_hat: 0.4, loss: 21.16\n",
            "    x: 3.5, y: 7.9, y_hat: 0.7, loss: 51.84\n",
            "    x: 4.0, y: 9.1, y_hat: 0.8, loss: 68.89\n",
            "    x: 5.0, y: 10.9, y_hat: 1.0, loss: 98.01\n",
            "    Loss = 129.4688\n",
            "\n",
            "(w=0.2, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.4, loss: 7.29\n",
            "    x: 1.2, y: 3.5, y_hat: 0.4, loss: 9.36\n",
            "    x: 2.0, y: 5.0, y_hat: 0.6, loss: 19.36\n",
            "    x: 3.5, y: 7.9, y_hat: 0.9, loss: 49.00\n",
            "    x: 4.0, y: 9.1, y_hat: 1.0, loss: 65.61\n",
            "    x: 5.0, y: 10.9, y_hat: 1.2, loss: 94.09\n",
            "    Loss = 122.3568\n",
            "\n",
            "(w=0.2, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.6, loss: 6.25\n",
            "    x: 1.2, y: 3.5, y_hat: 0.6, loss: 8.18\n",
            "    x: 2.0, y: 5.0, y_hat: 0.8, loss: 17.64\n",
            "    x: 3.5, y: 7.9, y_hat: 1.1, loss: 46.24\n",
            "    x: 4.0, y: 9.1, y_hat: 1.2, loss: 62.41\n",
            "    x: 5.0, y: 10.9, y_hat: 1.4, loss: 90.25\n",
            "    Loss = 115.4848\n",
            "\n",
            "(w=0.2, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.8, loss: 5.29\n",
            "    x: 1.2, y: 3.5, y_hat: 0.8, loss: 7.08\n",
            "    x: 2.0, y: 5.0, y_hat: 1.0, loss: 16.00\n",
            "    x: 3.5, y: 7.9, y_hat: 1.3, loss: 43.56\n",
            "    x: 4.0, y: 9.1, y_hat: 1.4, loss: 59.29\n",
            "    x: 5.0, y: 10.9, y_hat: 1.6, loss: 86.49\n",
            "    Loss = 108.8528\n",
            "\n",
            "(w=0.2, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.0, loss: 4.41\n",
            "    x: 1.2, y: 3.5, y_hat: 1.0, loss: 6.05\n",
            "    x: 2.0, y: 5.0, y_hat: 1.2, loss: 14.44\n",
            "    x: 3.5, y: 7.9, y_hat: 1.5, loss: 40.96\n",
            "    x: 4.0, y: 9.1, y_hat: 1.6, loss: 56.25\n",
            "    x: 5.0, y: 10.9, y_hat: 1.8, loss: 82.81\n",
            "    Loss = 102.4608\n",
            "\n",
            "(w=0.2, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.2, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 1.2, loss: 5.11\n",
            "    x: 2.0, y: 5.0, y_hat: 1.4, loss: 12.96\n",
            "    x: 3.5, y: 7.9, y_hat: 1.7, loss: 38.44\n",
            "    x: 4.0, y: 9.1, y_hat: 1.8, loss: 53.29\n",
            "    x: 5.0, y: 10.9, y_hat: 2.0, loss: 79.21\n",
            "    Loss = 96.3088\n",
            "\n",
            "(w=0.2, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 1.4, loss: 4.24\n",
            "    x: 2.0, y: 5.0, y_hat: 1.6, loss: 11.56\n",
            "    x: 3.5, y: 7.9, y_hat: 1.9, loss: 36.00\n",
            "    x: 4.0, y: 9.1, y_hat: 2.0, loss: 50.41\n",
            "    x: 5.0, y: 10.9, y_hat: 2.2, loss: 75.69\n",
            "    Loss = 90.3968\n",
            "\n",
            "(w=0.2, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 1.6, loss: 3.46\n",
            "    x: 2.0, y: 5.0, y_hat: 1.8, loss: 10.24\n",
            "    x: 3.5, y: 7.9, y_hat: 2.1, loss: 33.64\n",
            "    x: 4.0, y: 9.1, y_hat: 2.2, loss: 47.61\n",
            "    x: 5.0, y: 10.9, y_hat: 2.4, loss: 72.25\n",
            "    Loss = 84.7248\n",
            "\n",
            "(w=0.2, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 1.8, loss: 2.76\n",
            "    x: 2.0, y: 5.0, y_hat: 2.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 2.3, loss: 31.36\n",
            "    x: 4.0, y: 9.1, y_hat: 2.4, loss: 44.89\n",
            "    x: 5.0, y: 10.9, y_hat: 2.6, loss: 68.89\n",
            "    Loss = 79.2928\n",
            "\n",
            "(w=0.2, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 2.0, loss: 2.13\n",
            "    x: 2.0, y: 5.0, y_hat: 2.2, loss: 7.84\n",
            "    x: 3.5, y: 7.9, y_hat: 2.5, loss: 29.16\n",
            "    x: 4.0, y: 9.1, y_hat: 2.6, loss: 42.25\n",
            "    x: 5.0, y: 10.9, y_hat: 2.8, loss: 65.61\n",
            "    Loss = 74.1008\n",
            "\n",
            "(w=0.2, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 2.2, loss: 1.59\n",
            "    x: 2.0, y: 5.0, y_hat: 2.4, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 2.7, loss: 27.04\n",
            "    x: 4.0, y: 9.1, y_hat: 2.8, loss: 39.69\n",
            "    x: 5.0, y: 10.9, y_hat: 3.0, loss: 62.41\n",
            "    Loss = 69.1488\n",
            "\n",
            "(w=0.4, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.4, loss: 7.29\n",
            "    x: 1.2, y: 3.5, y_hat: 0.5, loss: 9.12\n",
            "    x: 2.0, y: 5.0, y_hat: 0.8, loss: 17.64\n",
            "    x: 3.5, y: 7.9, y_hat: 1.4, loss: 42.25\n",
            "    x: 4.0, y: 9.1, y_hat: 1.6, loss: 56.25\n",
            "    x: 5.0, y: 10.9, y_hat: 2.0, loss: 79.21\n",
            "    Loss = 105.8802\n",
            "\n",
            "(w=0.4, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.6, loss: 6.25\n",
            "    x: 1.2, y: 3.5, y_hat: 0.7, loss: 7.95\n",
            "    x: 2.0, y: 5.0, y_hat: 1.0, loss: 16.00\n",
            "    x: 3.5, y: 7.9, y_hat: 1.6, loss: 39.69\n",
            "    x: 4.0, y: 9.1, y_hat: 1.8, loss: 53.29\n",
            "    x: 5.0, y: 10.9, y_hat: 2.2, loss: 75.69\n",
            "    Loss = 99.4362\n",
            "\n",
            "(w=0.4, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.8, loss: 5.29\n",
            "    x: 1.2, y: 3.5, y_hat: 0.9, loss: 6.86\n",
            "    x: 2.0, y: 5.0, y_hat: 1.2, loss: 14.44\n",
            "    x: 3.5, y: 7.9, y_hat: 1.8, loss: 37.21\n",
            "    x: 4.0, y: 9.1, y_hat: 2.0, loss: 50.41\n",
            "    x: 5.0, y: 10.9, y_hat: 2.4, loss: 72.25\n",
            "    Loss = 93.2322\n",
            "\n",
            "(w=0.4, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.0, loss: 4.41\n",
            "    x: 1.2, y: 3.5, y_hat: 1.1, loss: 5.86\n",
            "    x: 2.0, y: 5.0, y_hat: 1.4, loss: 12.96\n",
            "    x: 3.5, y: 7.9, y_hat: 2.0, loss: 34.81\n",
            "    x: 4.0, y: 9.1, y_hat: 2.2, loss: 47.61\n",
            "    x: 5.0, y: 10.9, y_hat: 2.6, loss: 68.89\n",
            "    Loss = 87.2682\n",
            "\n",
            "(w=0.4, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.2, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 1.3, loss: 4.93\n",
            "    x: 2.0, y: 5.0, y_hat: 1.6, loss: 11.56\n",
            "    x: 3.5, y: 7.9, y_hat: 2.2, loss: 32.49\n",
            "    x: 4.0, y: 9.1, y_hat: 2.4, loss: 44.89\n",
            "    x: 5.0, y: 10.9, y_hat: 2.8, loss: 65.61\n",
            "    Loss = 81.5442\n",
            "\n",
            "(w=0.4, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 1.5, loss: 4.08\n",
            "    x: 2.0, y: 5.0, y_hat: 1.8, loss: 10.24\n",
            "    x: 3.5, y: 7.9, y_hat: 2.4, loss: 30.25\n",
            "    x: 4.0, y: 9.1, y_hat: 2.6, loss: 42.25\n",
            "    x: 5.0, y: 10.9, y_hat: 3.0, loss: 62.41\n",
            "    Loss = 76.0602\n",
            "\n",
            "(w=0.4, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 1.7, loss: 3.31\n",
            "    x: 2.0, y: 5.0, y_hat: 2.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 2.6, loss: 28.09\n",
            "    x: 4.0, y: 9.1, y_hat: 2.8, loss: 39.69\n",
            "    x: 5.0, y: 10.9, y_hat: 3.2, loss: 59.29\n",
            "    Loss = 70.8162\n",
            "\n",
            "(w=0.4, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 1.9, loss: 2.62\n",
            "    x: 2.0, y: 5.0, y_hat: 2.2, loss: 7.84\n",
            "    x: 3.5, y: 7.9, y_hat: 2.8, loss: 26.01\n",
            "    x: 4.0, y: 9.1, y_hat: 3.0, loss: 37.21\n",
            "    x: 5.0, y: 10.9, y_hat: 3.4, loss: 56.25\n",
            "    Loss = 65.8122\n",
            "\n",
            "(w=0.4, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 2.1, loss: 2.02\n",
            "    x: 2.0, y: 5.0, y_hat: 2.4, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 3.0, loss: 24.01\n",
            "    x: 4.0, y: 9.1, y_hat: 3.2, loss: 34.81\n",
            "    x: 5.0, y: 10.9, y_hat: 3.6, loss: 53.29\n",
            "    Loss = 61.0482\n",
            "\n",
            "(w=0.4, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 2.3, loss: 1.49\n",
            "    x: 2.0, y: 5.0, y_hat: 2.6, loss: 5.76\n",
            "    x: 3.5, y: 7.9, y_hat: 3.2, loss: 22.09\n",
            "    x: 4.0, y: 9.1, y_hat: 3.4, loss: 32.49\n",
            "    x: 5.0, y: 10.9, y_hat: 3.8, loss: 50.41\n",
            "    Loss = 56.5242\n",
            "\n",
            "(w=0.4, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 2.5, loss: 1.04\n",
            "    x: 2.0, y: 5.0, y_hat: 2.8, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 3.4, loss: 20.25\n",
            "    x: 4.0, y: 9.1, y_hat: 3.6, loss: 30.25\n",
            "    x: 5.0, y: 10.9, y_hat: 4.0, loss: 47.61\n",
            "    Loss = 52.2402\n",
            "\n",
            "(w=0.6, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.6, loss: 6.25\n",
            "    x: 1.2, y: 3.5, y_hat: 0.7, loss: 7.73\n",
            "    x: 2.0, y: 5.0, y_hat: 1.2, loss: 14.44\n",
            "    x: 3.5, y: 7.9, y_hat: 2.1, loss: 33.64\n",
            "    x: 4.0, y: 9.1, y_hat: 2.4, loss: 44.89\n",
            "    x: 5.0, y: 10.9, y_hat: 3.0, loss: 62.41\n",
            "    Loss = 84.6792\n",
            "\n",
            "(w=0.6, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.8, loss: 5.29\n",
            "    x: 1.2, y: 3.5, y_hat: 0.9, loss: 6.66\n",
            "    x: 2.0, y: 5.0, y_hat: 1.4, loss: 12.96\n",
            "    x: 3.5, y: 7.9, y_hat: 2.3, loss: 31.36\n",
            "    x: 4.0, y: 9.1, y_hat: 2.6, loss: 42.25\n",
            "    x: 5.0, y: 10.9, y_hat: 3.2, loss: 59.29\n",
            "    Loss = 78.9032\n",
            "\n",
            "(w=0.6, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.0, loss: 4.41\n",
            "    x: 1.2, y: 3.5, y_hat: 1.1, loss: 5.66\n",
            "    x: 2.0, y: 5.0, y_hat: 1.6, loss: 11.56\n",
            "    x: 3.5, y: 7.9, y_hat: 2.5, loss: 29.16\n",
            "    x: 4.0, y: 9.1, y_hat: 2.8, loss: 39.69\n",
            "    x: 5.0, y: 10.9, y_hat: 3.4, loss: 56.25\n",
            "    Loss = 73.3672\n",
            "\n",
            "(w=0.6, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.2, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 1.3, loss: 4.75\n",
            "    x: 2.0, y: 5.0, y_hat: 1.8, loss: 10.24\n",
            "    x: 3.5, y: 7.9, y_hat: 2.7, loss: 27.04\n",
            "    x: 4.0, y: 9.1, y_hat: 3.0, loss: 37.21\n",
            "    x: 5.0, y: 10.9, y_hat: 3.6, loss: 53.29\n",
            "    Loss = 68.0712\n",
            "\n",
            "(w=0.6, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 1.5, loss: 3.92\n",
            "    x: 2.0, y: 5.0, y_hat: 2.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 2.9, loss: 25.00\n",
            "    x: 4.0, y: 9.1, y_hat: 3.2, loss: 34.81\n",
            "    x: 5.0, y: 10.9, y_hat: 3.8, loss: 50.41\n",
            "    Loss = 63.0152\n",
            "\n",
            "(w=0.6, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 1.7, loss: 3.17\n",
            "    x: 2.0, y: 5.0, y_hat: 2.2, loss: 7.84\n",
            "    x: 3.5, y: 7.9, y_hat: 3.1, loss: 23.04\n",
            "    x: 4.0, y: 9.1, y_hat: 3.4, loss: 32.49\n",
            "    x: 5.0, y: 10.9, y_hat: 4.0, loss: 47.61\n",
            "    Loss = 58.1992\n",
            "\n",
            "(w=0.6, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 1.9, loss: 2.50\n",
            "    x: 2.0, y: 5.0, y_hat: 2.4, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 3.3, loss: 21.16\n",
            "    x: 4.0, y: 9.1, y_hat: 3.6, loss: 30.25\n",
            "    x: 5.0, y: 10.9, y_hat: 4.2, loss: 44.89\n",
            "    Loss = 53.6232\n",
            "\n",
            "(w=0.6, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 2.1, loss: 1.90\n",
            "    x: 2.0, y: 5.0, y_hat: 2.6, loss: 5.76\n",
            "    x: 3.5, y: 7.9, y_hat: 3.5, loss: 19.36\n",
            "    x: 4.0, y: 9.1, y_hat: 3.8, loss: 28.09\n",
            "    x: 5.0, y: 10.9, y_hat: 4.4, loss: 42.25\n",
            "    Loss = 49.2872\n",
            "\n",
            "(w=0.6, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 2.3, loss: 1.39\n",
            "    x: 2.0, y: 5.0, y_hat: 2.8, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 3.7, loss: 17.64\n",
            "    x: 4.0, y: 9.1, y_hat: 4.0, loss: 26.01\n",
            "    x: 5.0, y: 10.9, y_hat: 4.6, loss: 39.69\n",
            "    Loss = 45.1912\n",
            "\n",
            "(w=0.6, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 2.5, loss: 0.96\n",
            "    x: 2.0, y: 5.0, y_hat: 3.0, loss: 4.00\n",
            "    x: 3.5, y: 7.9, y_hat: 3.9, loss: 16.00\n",
            "    x: 4.0, y: 9.1, y_hat: 4.2, loss: 24.01\n",
            "    x: 5.0, y: 10.9, y_hat: 4.8, loss: 37.21\n",
            "    Loss = 41.3352\n",
            "\n",
            "(w=0.6, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 2.7, loss: 0.61\n",
            "    x: 2.0, y: 5.0, y_hat: 3.2, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 4.1, loss: 14.44\n",
            "    x: 4.0, y: 9.1, y_hat: 4.4, loss: 22.09\n",
            "    x: 5.0, y: 10.9, y_hat: 5.0, loss: 34.81\n",
            "    Loss = 37.7192\n",
            "\n",
            "(w=0.8, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.8, loss: 5.29\n",
            "    x: 1.2, y: 3.5, y_hat: 1.0, loss: 6.45\n",
            "    x: 2.0, y: 5.0, y_hat: 1.6, loss: 11.56\n",
            "    x: 3.5, y: 7.9, y_hat: 2.8, loss: 26.01\n",
            "    x: 4.0, y: 9.1, y_hat: 3.2, loss: 34.81\n",
            "    x: 5.0, y: 10.9, y_hat: 4.0, loss: 47.61\n",
            "    Loss = 65.8658\n",
            "\n",
            "(w=0.8, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.0, loss: 4.41\n",
            "    x: 1.2, y: 3.5, y_hat: 1.2, loss: 5.48\n",
            "    x: 2.0, y: 5.0, y_hat: 1.8, loss: 10.24\n",
            "    x: 3.5, y: 7.9, y_hat: 3.0, loss: 24.01\n",
            "    x: 4.0, y: 9.1, y_hat: 3.4, loss: 32.49\n",
            "    x: 5.0, y: 10.9, y_hat: 4.2, loss: 44.89\n",
            "    Loss = 60.7578\n",
            "\n",
            "(w=0.8, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.2, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 1.4, loss: 4.58\n",
            "    x: 2.0, y: 5.0, y_hat: 2.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 3.2, loss: 22.09\n",
            "    x: 4.0, y: 9.1, y_hat: 3.6, loss: 30.25\n",
            "    x: 5.0, y: 10.9, y_hat: 4.4, loss: 42.25\n",
            "    Loss = 55.8898\n",
            "\n",
            "(w=0.8, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 1.6, loss: 3.76\n",
            "    x: 2.0, y: 5.0, y_hat: 2.2, loss: 7.84\n",
            "    x: 3.5, y: 7.9, y_hat: 3.4, loss: 20.25\n",
            "    x: 4.0, y: 9.1, y_hat: 3.8, loss: 28.09\n",
            "    x: 5.0, y: 10.9, y_hat: 4.6, loss: 39.69\n",
            "    Loss = 51.2618\n",
            "\n",
            "(w=0.8, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 1.8, loss: 3.03\n",
            "    x: 2.0, y: 5.0, y_hat: 2.4, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 3.6, loss: 18.49\n",
            "    x: 4.0, y: 9.1, y_hat: 4.0, loss: 26.01\n",
            "    x: 5.0, y: 10.9, y_hat: 4.8, loss: 37.21\n",
            "    Loss = 46.8738\n",
            "\n",
            "(w=0.8, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 2.0, loss: 2.37\n",
            "    x: 2.0, y: 5.0, y_hat: 2.6, loss: 5.76\n",
            "    x: 3.5, y: 7.9, y_hat: 3.8, loss: 16.81\n",
            "    x: 4.0, y: 9.1, y_hat: 4.2, loss: 24.01\n",
            "    x: 5.0, y: 10.9, y_hat: 5.0, loss: 34.81\n",
            "    Loss = 42.7258\n",
            "\n",
            "(w=0.8, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 2.2, loss: 1.80\n",
            "    x: 2.0, y: 5.0, y_hat: 2.8, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 4.0, loss: 15.21\n",
            "    x: 4.0, y: 9.1, y_hat: 4.4, loss: 22.09\n",
            "    x: 5.0, y: 10.9, y_hat: 5.2, loss: 32.49\n",
            "    Loss = 38.8178\n",
            "\n",
            "(w=0.8, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 2.4, loss: 1.30\n",
            "    x: 2.0, y: 5.0, y_hat: 3.0, loss: 4.00\n",
            "    x: 3.5, y: 7.9, y_hat: 4.2, loss: 13.69\n",
            "    x: 4.0, y: 9.1, y_hat: 4.6, loss: 20.25\n",
            "    x: 5.0, y: 10.9, y_hat: 5.4, loss: 30.25\n",
            "    Loss = 35.1498\n",
            "\n",
            "(w=0.8, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 2.6, loss: 0.88\n",
            "    x: 2.0, y: 5.0, y_hat: 3.2, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 4.4, loss: 12.25\n",
            "    x: 4.0, y: 9.1, y_hat: 4.8, loss: 18.49\n",
            "    x: 5.0, y: 10.9, y_hat: 5.6, loss: 28.09\n",
            "    Loss = 31.7218\n",
            "\n",
            "(w=0.8, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 2.8, loss: 0.55\n",
            "    x: 2.0, y: 5.0, y_hat: 3.4, loss: 2.56\n",
            "    x: 3.5, y: 7.9, y_hat: 4.6, loss: 10.89\n",
            "    x: 4.0, y: 9.1, y_hat: 5.0, loss: 16.81\n",
            "    x: 5.0, y: 10.9, y_hat: 5.8, loss: 26.01\n",
            "    Loss = 28.5338\n",
            "\n",
            "(w=0.8, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.0, loss: 0.29\n",
            "    x: 2.0, y: 5.0, y_hat: 3.6, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 4.8, loss: 9.61\n",
            "    x: 4.0, y: 9.1, y_hat: 5.2, loss: 15.21\n",
            "    x: 5.0, y: 10.9, y_hat: 6.0, loss: 24.01\n",
            "    Loss = 25.5858\n",
            "\n",
            "(w=1.0, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.0, loss: 4.41\n",
            "    x: 1.2, y: 3.5, y_hat: 1.2, loss: 5.29\n",
            "    x: 2.0, y: 5.0, y_hat: 2.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 3.5, loss: 19.36\n",
            "    x: 4.0, y: 9.1, y_hat: 4.0, loss: 26.01\n",
            "    x: 5.0, y: 10.9, y_hat: 5.0, loss: 34.81\n",
            "    Loss = 49.4400\n",
            "\n",
            "(w=1.0, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.2, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 1.4, loss: 4.41\n",
            "    x: 2.0, y: 5.0, y_hat: 2.2, loss: 7.84\n",
            "    x: 3.5, y: 7.9, y_hat: 3.7, loss: 17.64\n",
            "    x: 4.0, y: 9.1, y_hat: 4.2, loss: 24.01\n",
            "    x: 5.0, y: 10.9, y_hat: 5.2, loss: 32.49\n",
            "    Loss = 45.0000\n",
            "\n",
            "(w=1.0, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 1.6, loss: 3.61\n",
            "    x: 2.0, y: 5.0, y_hat: 2.4, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 3.9, loss: 16.00\n",
            "    x: 4.0, y: 9.1, y_hat: 4.4, loss: 22.09\n",
            "    x: 5.0, y: 10.9, y_hat: 5.4, loss: 30.25\n",
            "    Loss = 40.8000\n",
            "\n",
            "(w=1.0, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 1.8, loss: 2.89\n",
            "    x: 2.0, y: 5.0, y_hat: 2.6, loss: 5.76\n",
            "    x: 3.5, y: 7.9, y_hat: 4.1, loss: 14.44\n",
            "    x: 4.0, y: 9.1, y_hat: 4.6, loss: 20.25\n",
            "    x: 5.0, y: 10.9, y_hat: 5.6, loss: 28.09\n",
            "    Loss = 36.8400\n",
            "\n",
            "(w=1.0, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 2.0, loss: 2.25\n",
            "    x: 2.0, y: 5.0, y_hat: 2.8, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 4.3, loss: 12.96\n",
            "    x: 4.0, y: 9.1, y_hat: 4.8, loss: 18.49\n",
            "    x: 5.0, y: 10.9, y_hat: 5.8, loss: 26.01\n",
            "    Loss = 33.1200\n",
            "\n",
            "(w=1.0, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 2.2, loss: 1.69\n",
            "    x: 2.0, y: 5.0, y_hat: 3.0, loss: 4.00\n",
            "    x: 3.5, y: 7.9, y_hat: 4.5, loss: 11.56\n",
            "    x: 4.0, y: 9.1, y_hat: 5.0, loss: 16.81\n",
            "    x: 5.0, y: 10.9, y_hat: 6.0, loss: 24.01\n",
            "    Loss = 29.6400\n",
            "\n",
            "(w=1.0, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 2.4, loss: 1.21\n",
            "    x: 2.0, y: 5.0, y_hat: 3.2, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 4.7, loss: 10.24\n",
            "    x: 4.0, y: 9.1, y_hat: 5.2, loss: 15.21\n",
            "    x: 5.0, y: 10.9, y_hat: 6.2, loss: 22.09\n",
            "    Loss = 26.4000\n",
            "\n",
            "(w=1.0, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 2.6, loss: 0.81\n",
            "    x: 2.0, y: 5.0, y_hat: 3.4, loss: 2.56\n",
            "    x: 3.5, y: 7.9, y_hat: 4.9, loss: 9.00\n",
            "    x: 4.0, y: 9.1, y_hat: 5.4, loss: 13.69\n",
            "    x: 5.0, y: 10.9, y_hat: 6.4, loss: 20.25\n",
            "    Loss = 23.4000\n",
            "\n",
            "(w=1.0, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 2.8, loss: 0.49\n",
            "    x: 2.0, y: 5.0, y_hat: 3.6, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 5.1, loss: 7.84\n",
            "    x: 4.0, y: 9.1, y_hat: 5.6, loss: 12.25\n",
            "    x: 5.0, y: 10.9, y_hat: 6.6, loss: 18.49\n",
            "    Loss = 20.6400\n",
            "\n",
            "(w=1.0, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.0, loss: 0.25\n",
            "    x: 2.0, y: 5.0, y_hat: 3.8, loss: 1.44\n",
            "    x: 3.5, y: 7.9, y_hat: 5.3, loss: 6.76\n",
            "    x: 4.0, y: 9.1, y_hat: 5.8, loss: 10.89\n",
            "    x: 5.0, y: 10.9, y_hat: 6.8, loss: 16.81\n",
            "    Loss = 18.1200\n",
            "\n",
            "(w=1.0, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.2, loss: 0.09\n",
            "    x: 2.0, y: 5.0, y_hat: 4.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 5.5, loss: 5.76\n",
            "    x: 4.0, y: 9.1, y_hat: 6.0, loss: 9.61\n",
            "    x: 5.0, y: 10.9, y_hat: 7.0, loss: 15.21\n",
            "    Loss = 15.8400\n",
            "\n",
            "(w=1.2, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.2, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 1.4, loss: 4.24\n",
            "    x: 2.0, y: 5.0, y_hat: 2.4, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 4.2, loss: 13.69\n",
            "    x: 4.0, y: 9.1, y_hat: 4.8, loss: 18.49\n",
            "    x: 5.0, y: 10.9, y_hat: 6.0, loss: 24.01\n",
            "    Loss = 35.4018\n",
            "\n",
            "(w=1.2, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 1.6, loss: 3.46\n",
            "    x: 2.0, y: 5.0, y_hat: 2.6, loss: 5.76\n",
            "    x: 3.5, y: 7.9, y_hat: 4.4, loss: 12.25\n",
            "    x: 4.0, y: 9.1, y_hat: 5.0, loss: 16.81\n",
            "    x: 5.0, y: 10.9, y_hat: 6.2, loss: 22.09\n",
            "    Loss = 31.6298\n",
            "\n",
            "(w=1.2, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 1.8, loss: 2.76\n",
            "    x: 2.0, y: 5.0, y_hat: 2.8, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 4.6, loss: 10.89\n",
            "    x: 4.0, y: 9.1, y_hat: 5.2, loss: 15.21\n",
            "    x: 5.0, y: 10.9, y_hat: 6.4, loss: 20.25\n",
            "    Loss = 28.0978\n",
            "\n",
            "(w=1.2, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 2.0, loss: 2.13\n",
            "    x: 2.0, y: 5.0, y_hat: 3.0, loss: 4.00\n",
            "    x: 3.5, y: 7.9, y_hat: 4.8, loss: 9.61\n",
            "    x: 4.0, y: 9.1, y_hat: 5.4, loss: 13.69\n",
            "    x: 5.0, y: 10.9, y_hat: 6.6, loss: 18.49\n",
            "    Loss = 24.8058\n",
            "\n",
            "(w=1.2, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 2.2, loss: 1.59\n",
            "    x: 2.0, y: 5.0, y_hat: 3.2, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 5.0, loss: 8.41\n",
            "    x: 4.0, y: 9.1, y_hat: 5.6, loss: 12.25\n",
            "    x: 5.0, y: 10.9, y_hat: 6.8, loss: 16.81\n",
            "    Loss = 21.7538\n",
            "\n",
            "(w=1.2, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 2.4, loss: 1.12\n",
            "    x: 2.0, y: 5.0, y_hat: 3.4, loss: 2.56\n",
            "    x: 3.5, y: 7.9, y_hat: 5.2, loss: 7.29\n",
            "    x: 4.0, y: 9.1, y_hat: 5.8, loss: 10.89\n",
            "    x: 5.0, y: 10.9, y_hat: 7.0, loss: 15.21\n",
            "    Loss = 18.9418\n",
            "\n",
            "(w=1.2, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 2.6, loss: 0.74\n",
            "    x: 2.0, y: 5.0, y_hat: 3.6, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 5.4, loss: 6.25\n",
            "    x: 4.0, y: 9.1, y_hat: 6.0, loss: 9.61\n",
            "    x: 5.0, y: 10.9, y_hat: 7.2, loss: 13.69\n",
            "    Loss = 16.3698\n",
            "\n",
            "(w=1.2, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 2.8, loss: 0.44\n",
            "    x: 2.0, y: 5.0, y_hat: 3.8, loss: 1.44\n",
            "    x: 3.5, y: 7.9, y_hat: 5.6, loss: 5.29\n",
            "    x: 4.0, y: 9.1, y_hat: 6.2, loss: 8.41\n",
            "    x: 5.0, y: 10.9, y_hat: 7.4, loss: 12.25\n",
            "    Loss = 14.0378\n",
            "\n",
            "(w=1.2, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.0, loss: 0.21\n",
            "    x: 2.0, y: 5.0, y_hat: 4.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 5.8, loss: 4.41\n",
            "    x: 4.0, y: 9.1, y_hat: 6.4, loss: 7.29\n",
            "    x: 5.0, y: 10.9, y_hat: 7.6, loss: 10.89\n",
            "    Loss = 11.9458\n",
            "\n",
            "(w=1.2, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.2, loss: 0.07\n",
            "    x: 2.0, y: 5.0, y_hat: 4.2, loss: 0.64\n",
            "    x: 3.5, y: 7.9, y_hat: 6.0, loss: 3.61\n",
            "    x: 4.0, y: 9.1, y_hat: 6.6, loss: 6.25\n",
            "    x: 5.0, y: 10.9, y_hat: 7.8, loss: 9.61\n",
            "    Loss = 10.0938\n",
            "\n",
            "(w=1.2, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.4, loss: 0.00\n",
            "    x: 2.0, y: 5.0, y_hat: 4.4, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 6.2, loss: 2.89\n",
            "    x: 4.0, y: 9.1, y_hat: 6.8, loss: 5.29\n",
            "    x: 5.0, y: 10.9, y_hat: 8.0, loss: 8.41\n",
            "    Loss = 8.4818\n",
            "\n",
            "(w=1.4, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 1.7, loss: 3.31\n",
            "    x: 2.0, y: 5.0, y_hat: 2.8, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 4.9, loss: 9.00\n",
            "    x: 4.0, y: 9.1, y_hat: 5.6, loss: 12.25\n",
            "    x: 5.0, y: 10.9, y_hat: 7.0, loss: 15.21\n",
            "    Loss = 23.7512\n",
            "\n",
            "(w=1.4, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 1.9, loss: 2.62\n",
            "    x: 2.0, y: 5.0, y_hat: 3.0, loss: 4.00\n",
            "    x: 3.5, y: 7.9, y_hat: 5.1, loss: 7.84\n",
            "    x: 4.0, y: 9.1, y_hat: 5.8, loss: 10.89\n",
            "    x: 5.0, y: 10.9, y_hat: 7.2, loss: 13.69\n",
            "    Loss = 20.6472\n",
            "\n",
            "(w=1.4, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 2.1, loss: 2.02\n",
            "    x: 2.0, y: 5.0, y_hat: 3.2, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 5.3, loss: 6.76\n",
            "    x: 4.0, y: 9.1, y_hat: 6.0, loss: 9.61\n",
            "    x: 5.0, y: 10.9, y_hat: 7.4, loss: 12.25\n",
            "    Loss = 17.7832\n",
            "\n",
            "(w=1.4, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 2.3, loss: 1.49\n",
            "    x: 2.0, y: 5.0, y_hat: 3.4, loss: 2.56\n",
            "    x: 3.5, y: 7.9, y_hat: 5.5, loss: 5.76\n",
            "    x: 4.0, y: 9.1, y_hat: 6.2, loss: 8.41\n",
            "    x: 5.0, y: 10.9, y_hat: 7.6, loss: 10.89\n",
            "    Loss = 15.1592\n",
            "\n",
            "(w=1.4, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 2.5, loss: 1.04\n",
            "    x: 2.0, y: 5.0, y_hat: 3.6, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 5.7, loss: 4.84\n",
            "    x: 4.0, y: 9.1, y_hat: 6.4, loss: 7.29\n",
            "    x: 5.0, y: 10.9, y_hat: 7.8, loss: 9.61\n",
            "    Loss = 12.7752\n",
            "\n",
            "(w=1.4, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 2.7, loss: 0.67\n",
            "    x: 2.0, y: 5.0, y_hat: 3.8, loss: 1.44\n",
            "    x: 3.5, y: 7.9, y_hat: 5.9, loss: 4.00\n",
            "    x: 4.0, y: 9.1, y_hat: 6.6, loss: 6.25\n",
            "    x: 5.0, y: 10.9, y_hat: 8.0, loss: 8.41\n",
            "    Loss = 10.6312\n",
            "\n",
            "(w=1.4, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 2.9, loss: 0.38\n",
            "    x: 2.0, y: 5.0, y_hat: 4.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 6.1, loss: 3.24\n",
            "    x: 4.0, y: 9.1, y_hat: 6.8, loss: 5.29\n",
            "    x: 5.0, y: 10.9, y_hat: 8.2, loss: 7.29\n",
            "    Loss = 8.7272\n",
            "\n",
            "(w=1.4, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.1, loss: 0.18\n",
            "    x: 2.0, y: 5.0, y_hat: 4.2, loss: 0.64\n",
            "    x: 3.5, y: 7.9, y_hat: 6.3, loss: 2.56\n",
            "    x: 4.0, y: 9.1, y_hat: 7.0, loss: 4.41\n",
            "    x: 5.0, y: 10.9, y_hat: 8.4, loss: 6.25\n",
            "    Loss = 7.0632\n",
            "\n",
            "(w=1.4, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.3, loss: 0.05\n",
            "    x: 2.0, y: 5.0, y_hat: 4.4, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 6.5, loss: 1.96\n",
            "    x: 4.0, y: 9.1, y_hat: 7.2, loss: 3.61\n",
            "    x: 5.0, y: 10.9, y_hat: 8.6, loss: 5.29\n",
            "    Loss = 5.6392\n",
            "\n",
            "(w=1.4, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.5, loss: 0.00\n",
            "    x: 2.0, y: 5.0, y_hat: 4.6, loss: 0.16\n",
            "    x: 3.5, y: 7.9, y_hat: 6.7, loss: 1.44\n",
            "    x: 4.0, y: 9.1, y_hat: 7.4, loss: 2.89\n",
            "    x: 5.0, y: 10.9, y_hat: 8.8, loss: 4.41\n",
            "    Loss = 4.4552\n",
            "\n",
            "(w=1.4, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.7, loss: 0.03\n",
            "    x: 2.0, y: 5.0, y_hat: 4.8, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 6.9, loss: 1.00\n",
            "    x: 4.0, y: 9.1, y_hat: 7.6, loss: 2.25\n",
            "    x: 5.0, y: 10.9, y_hat: 9.0, loss: 3.61\n",
            "    Loss = 3.5112\n",
            "\n",
            "(w=1.6, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 1.9, loss: 2.50\n",
            "    x: 2.0, y: 5.0, y_hat: 3.2, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 5.6, loss: 5.29\n",
            "    x: 4.0, y: 9.1, y_hat: 6.4, loss: 7.29\n",
            "    x: 5.0, y: 10.9, y_hat: 8.0, loss: 8.41\n",
            "    Loss = 14.4882\n",
            "\n",
            "(w=1.6, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 2.1, loss: 1.90\n",
            "    x: 2.0, y: 5.0, y_hat: 3.4, loss: 2.56\n",
            "    x: 3.5, y: 7.9, y_hat: 5.8, loss: 4.41\n",
            "    x: 4.0, y: 9.1, y_hat: 6.6, loss: 6.25\n",
            "    x: 5.0, y: 10.9, y_hat: 8.2, loss: 7.29\n",
            "    Loss = 12.0522\n",
            "\n",
            "(w=1.6, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 2.3, loss: 1.39\n",
            "    x: 2.0, y: 5.0, y_hat: 3.6, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 6.0, loss: 3.61\n",
            "    x: 4.0, y: 9.1, y_hat: 6.8, loss: 5.29\n",
            "    x: 5.0, y: 10.9, y_hat: 8.4, loss: 6.25\n",
            "    Loss = 9.8562\n",
            "\n",
            "(w=1.6, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 2.5, loss: 0.96\n",
            "    x: 2.0, y: 5.0, y_hat: 3.8, loss: 1.44\n",
            "    x: 3.5, y: 7.9, y_hat: 6.2, loss: 2.89\n",
            "    x: 4.0, y: 9.1, y_hat: 7.0, loss: 4.41\n",
            "    x: 5.0, y: 10.9, y_hat: 8.6, loss: 5.29\n",
            "    Loss = 7.9002\n",
            "\n",
            "(w=1.6, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 2.7, loss: 0.61\n",
            "    x: 2.0, y: 5.0, y_hat: 4.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 6.4, loss: 2.25\n",
            "    x: 4.0, y: 9.1, y_hat: 7.2, loss: 3.61\n",
            "    x: 5.0, y: 10.9, y_hat: 8.8, loss: 4.41\n",
            "    Loss = 6.1842\n",
            "\n",
            "(w=1.6, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 2.9, loss: 0.34\n",
            "    x: 2.0, y: 5.0, y_hat: 4.2, loss: 0.64\n",
            "    x: 3.5, y: 7.9, y_hat: 6.6, loss: 1.69\n",
            "    x: 4.0, y: 9.1, y_hat: 7.4, loss: 2.89\n",
            "    x: 5.0, y: 10.9, y_hat: 9.0, loss: 3.61\n",
            "    Loss = 4.7082\n",
            "\n",
            "(w=1.6, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.1, loss: 0.14\n",
            "    x: 2.0, y: 5.0, y_hat: 4.4, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 6.8, loss: 1.21\n",
            "    x: 4.0, y: 9.1, y_hat: 7.6, loss: 2.25\n",
            "    x: 5.0, y: 10.9, y_hat: 9.2, loss: 2.89\n",
            "    Loss = 3.4722\n",
            "\n",
            "(w=1.6, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.3, loss: 0.03\n",
            "    x: 2.0, y: 5.0, y_hat: 4.6, loss: 0.16\n",
            "    x: 3.5, y: 7.9, y_hat: 7.0, loss: 0.81\n",
            "    x: 4.0, y: 9.1, y_hat: 7.8, loss: 1.69\n",
            "    x: 5.0, y: 10.9, y_hat: 9.4, loss: 2.25\n",
            "    Loss = 2.4762\n",
            "\n",
            "(w=1.6, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.5, loss: 0.00\n",
            "    x: 2.0, y: 5.0, y_hat: 4.8, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 7.2, loss: 0.49\n",
            "    x: 4.0, y: 9.1, y_hat: 8.0, loss: 1.21\n",
            "    x: 5.0, y: 10.9, y_hat: 9.6, loss: 1.69\n",
            "    Loss = 1.7202\n",
            "\n",
            "(w=1.6, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.7, loss: 0.05\n",
            "    x: 2.0, y: 5.0, y_hat: 5.0, loss: 0.00\n",
            "    x: 3.5, y: 7.9, y_hat: 7.4, loss: 0.25\n",
            "    x: 4.0, y: 9.1, y_hat: 8.2, loss: 0.81\n",
            "    x: 5.0, y: 10.9, y_hat: 9.8, loss: 1.21\n",
            "    Loss = 1.2042\n",
            "\n",
            "(w=1.6, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 3.9, loss: 0.18\n",
            "    x: 2.0, y: 5.0, y_hat: 5.2, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 7.6, loss: 0.09\n",
            "    x: 4.0, y: 9.1, y_hat: 8.4, loss: 0.49\n",
            "    x: 5.0, y: 10.9, y_hat: 10.0, loss: 0.81\n",
            "    Loss = 0.9282\n",
            "\n",
            "(w=1.8, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 2.2, loss: 1.80\n",
            "    x: 2.0, y: 5.0, y_hat: 3.6, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 6.3, loss: 2.56\n",
            "    x: 4.0, y: 9.1, y_hat: 7.2, loss: 3.61\n",
            "    x: 5.0, y: 10.9, y_hat: 9.0, loss: 3.61\n",
            "    Loss = 7.6128\n",
            "\n",
            "(w=1.8, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 2.4, loss: 1.30\n",
            "    x: 2.0, y: 5.0, y_hat: 3.8, loss: 1.44\n",
            "    x: 3.5, y: 7.9, y_hat: 6.5, loss: 1.96\n",
            "    x: 4.0, y: 9.1, y_hat: 7.4, loss: 2.89\n",
            "    x: 5.0, y: 10.9, y_hat: 9.2, loss: 2.89\n",
            "    Loss = 5.8448\n",
            "\n",
            "(w=1.8, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 2.6, loss: 0.88\n",
            "    x: 2.0, y: 5.0, y_hat: 4.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 6.7, loss: 1.44\n",
            "    x: 4.0, y: 9.1, y_hat: 7.6, loss: 2.25\n",
            "    x: 5.0, y: 10.9, y_hat: 9.4, loss: 2.25\n",
            "    Loss = 4.3168\n",
            "\n",
            "(w=1.8, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 2.8, loss: 0.55\n",
            "    x: 2.0, y: 5.0, y_hat: 4.2, loss: 0.64\n",
            "    x: 3.5, y: 7.9, y_hat: 6.9, loss: 1.00\n",
            "    x: 4.0, y: 9.1, y_hat: 7.8, loss: 1.69\n",
            "    x: 5.0, y: 10.9, y_hat: 9.6, loss: 1.69\n",
            "    Loss = 3.0288\n",
            "\n",
            "(w=1.8, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 3.0, loss: 0.29\n",
            "    x: 2.0, y: 5.0, y_hat: 4.4, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 7.1, loss: 0.64\n",
            "    x: 4.0, y: 9.1, y_hat: 8.0, loss: 1.21\n",
            "    x: 5.0, y: 10.9, y_hat: 9.8, loss: 1.21\n",
            "    Loss = 1.9808\n",
            "\n",
            "(w=1.8, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.2, loss: 0.12\n",
            "    x: 2.0, y: 5.0, y_hat: 4.6, loss: 0.16\n",
            "    x: 3.5, y: 7.9, y_hat: 7.3, loss: 0.36\n",
            "    x: 4.0, y: 9.1, y_hat: 8.2, loss: 0.81\n",
            "    x: 5.0, y: 10.9, y_hat: 10.0, loss: 0.81\n",
            "    Loss = 1.1728\n",
            "\n",
            "(w=1.8, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.4, loss: 0.02\n",
            "    x: 2.0, y: 5.0, y_hat: 4.8, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 7.5, loss: 0.16\n",
            "    x: 4.0, y: 9.1, y_hat: 8.4, loss: 0.49\n",
            "    x: 5.0, y: 10.9, y_hat: 10.2, loss: 0.49\n",
            "    Loss = 0.6048\n",
            "\n",
            "(w=1.8, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.6, loss: 0.00\n",
            "    x: 2.0, y: 5.0, y_hat: 5.0, loss: 0.00\n",
            "    x: 3.5, y: 7.9, y_hat: 7.7, loss: 0.04\n",
            "    x: 4.0, y: 9.1, y_hat: 8.6, loss: 0.25\n",
            "    x: 5.0, y: 10.9, y_hat: 10.4, loss: 0.25\n",
            "    Loss = 0.2768\n",
            "\n",
            "(w=1.8, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.8, loss: 0.07\n",
            "    x: 2.0, y: 5.0, y_hat: 5.2, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 7.9, loss: 0.00\n",
            "    x: 4.0, y: 9.1, y_hat: 8.8, loss: 0.09\n",
            "    x: 5.0, y: 10.9, y_hat: 10.6, loss: 0.09\n",
            "    Loss = 0.1888\n",
            "\n",
            "(w=1.8, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 4.0, loss: 0.21\n",
            "    x: 2.0, y: 5.0, y_hat: 5.4, loss: 0.16\n",
            "    x: 3.5, y: 7.9, y_hat: 8.1, loss: 0.04\n",
            "    x: 4.0, y: 9.1, y_hat: 9.0, loss: 0.01\n",
            "    x: 5.0, y: 10.9, y_hat: 10.8, loss: 0.01\n",
            "    Loss = 0.3408\n",
            "\n",
            "(w=1.8, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 4.2, loss: 0.44\n",
            "    x: 2.0, y: 5.0, y_hat: 5.6, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 8.3, loss: 0.16\n",
            "    x: 4.0, y: 9.1, y_hat: 9.2, loss: 0.01\n",
            "    x: 5.0, y: 10.9, y_hat: 11.0, loss: 0.01\n",
            "    Loss = 0.7328\n",
            "\n",
            "(w=2.0, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 2.4, loss: 1.21\n",
            "    x: 2.0, y: 5.0, y_hat: 4.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 7.0, loss: 0.81\n",
            "    x: 4.0, y: 9.1, y_hat: 8.0, loss: 1.21\n",
            "    x: 5.0, y: 10.9, y_hat: 10.0, loss: 0.81\n",
            "    Loss = 3.1250\n",
            "\n",
            "(w=2.0, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 2.6, loss: 0.81\n",
            "    x: 2.0, y: 5.0, y_hat: 4.2, loss: 0.64\n",
            "    x: 3.5, y: 7.9, y_hat: 7.2, loss: 0.49\n",
            "    x: 4.0, y: 9.1, y_hat: 8.2, loss: 0.81\n",
            "    x: 5.0, y: 10.9, y_hat: 10.2, loss: 0.49\n",
            "    Loss = 2.0250\n",
            "\n",
            "(w=2.0, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 2.8, loss: 0.49\n",
            "    x: 2.0, y: 5.0, y_hat: 4.4, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 7.4, loss: 0.25\n",
            "    x: 4.0, y: 9.1, y_hat: 8.4, loss: 0.49\n",
            "    x: 5.0, y: 10.9, y_hat: 10.4, loss: 0.25\n",
            "    Loss = 1.1650\n",
            "\n",
            "(w=2.0, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 3.0, loss: 0.25\n",
            "    x: 2.0, y: 5.0, y_hat: 4.6, loss: 0.16\n",
            "    x: 3.5, y: 7.9, y_hat: 7.6, loss: 0.09\n",
            "    x: 4.0, y: 9.1, y_hat: 8.6, loss: 0.25\n",
            "    x: 5.0, y: 10.9, y_hat: 10.6, loss: 0.09\n",
            "    Loss = 0.5450\n",
            "\n",
            "(w=2.0, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.2, loss: 0.09\n",
            "    x: 2.0, y: 5.0, y_hat: 4.8, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 7.8, loss: 0.01\n",
            "    x: 4.0, y: 9.1, y_hat: 8.8, loss: 0.09\n",
            "    x: 5.0, y: 10.9, y_hat: 10.8, loss: 0.01\n",
            "    Loss = 0.1650\n",
            "\n",
            "(w=2.0, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.4, loss: 0.01\n",
            "    x: 2.0, y: 5.0, y_hat: 5.0, loss: 0.00\n",
            "    x: 3.5, y: 7.9, y_hat: 8.0, loss: 0.01\n",
            "    x: 4.0, y: 9.1, y_hat: 9.0, loss: 0.01\n",
            "    x: 5.0, y: 10.9, y_hat: 11.0, loss: 0.01\n",
            "    Loss = 0.0250\n",
            "\n",
            "(w=2.0, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.6, loss: 0.01\n",
            "    x: 2.0, y: 5.0, y_hat: 5.2, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 8.2, loss: 0.09\n",
            "    x: 4.0, y: 9.1, y_hat: 9.2, loss: 0.01\n",
            "    x: 5.0, y: 10.9, y_hat: 11.2, loss: 0.09\n",
            "    Loss = 0.1250\n",
            "\n",
            "(w=2.0, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.8, loss: 0.09\n",
            "    x: 2.0, y: 5.0, y_hat: 5.4, loss: 0.16\n",
            "    x: 3.5, y: 7.9, y_hat: 8.4, loss: 0.25\n",
            "    x: 4.0, y: 9.1, y_hat: 9.4, loss: 0.09\n",
            "    x: 5.0, y: 10.9, y_hat: 11.4, loss: 0.25\n",
            "    Loss = 0.4650\n",
            "\n",
            "(w=2.0, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 4.0, loss: 0.25\n",
            "    x: 2.0, y: 5.0, y_hat: 5.6, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 8.6, loss: 0.49\n",
            "    x: 4.0, y: 9.1, y_hat: 9.6, loss: 0.25\n",
            "    x: 5.0, y: 10.9, y_hat: 11.6, loss: 0.49\n",
            "    Loss = 1.0450\n",
            "\n",
            "(w=2.0, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 4.2, loss: 0.49\n",
            "    x: 2.0, y: 5.0, y_hat: 5.8, loss: 0.64\n",
            "    x: 3.5, y: 7.9, y_hat: 8.8, loss: 0.81\n",
            "    x: 4.0, y: 9.1, y_hat: 9.8, loss: 0.49\n",
            "    x: 5.0, y: 10.9, y_hat: 11.8, loss: 0.81\n",
            "    Loss = 1.8650\n",
            "\n",
            "(w=2.0, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 4.4, loss: 0.81\n",
            "    x: 2.0, y: 5.0, y_hat: 6.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 9.0, loss: 1.21\n",
            "    x: 4.0, y: 9.1, y_hat: 10.0, loss: 0.81\n",
            "    x: 5.0, y: 10.9, y_hat: 12.0, loss: 1.21\n",
            "    Loss = 2.9250\n",
            "\n",
            "(w=2.2, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 2.6, loss: 0.74\n",
            "    x: 2.0, y: 5.0, y_hat: 4.4, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 7.7, loss: 0.04\n",
            "    x: 4.0, y: 9.1, y_hat: 8.8, loss: 0.09\n",
            "    x: 5.0, y: 10.9, y_hat: 11.0, loss: 0.01\n",
            "    Loss = 1.0248\n",
            "\n",
            "(w=2.2, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 2.8, loss: 0.44\n",
            "    x: 2.0, y: 5.0, y_hat: 4.6, loss: 0.16\n",
            "    x: 3.5, y: 7.9, y_hat: 7.9, loss: 0.00\n",
            "    x: 4.0, y: 9.1, y_hat: 9.0, loss: 0.01\n",
            "    x: 5.0, y: 10.9, y_hat: 11.2, loss: 0.09\n",
            "    Loss = 0.5928\n",
            "\n",
            "(w=2.2, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 3.0, loss: 0.21\n",
            "    x: 2.0, y: 5.0, y_hat: 4.8, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 8.1, loss: 0.04\n",
            "    x: 4.0, y: 9.1, y_hat: 9.2, loss: 0.01\n",
            "    x: 5.0, y: 10.9, y_hat: 11.4, loss: 0.25\n",
            "    Loss = 0.4008\n",
            "\n",
            "(w=2.2, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.2, loss: 0.07\n",
            "    x: 2.0, y: 5.0, y_hat: 5.0, loss: 0.00\n",
            "    x: 3.5, y: 7.9, y_hat: 8.3, loss: 0.16\n",
            "    x: 4.0, y: 9.1, y_hat: 9.4, loss: 0.09\n",
            "    x: 5.0, y: 10.9, y_hat: 11.6, loss: 0.49\n",
            "    Loss = 0.4488\n",
            "\n",
            "(w=2.2, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.4, loss: 0.00\n",
            "    x: 2.0, y: 5.0, y_hat: 5.2, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 8.5, loss: 0.36\n",
            "    x: 4.0, y: 9.1, y_hat: 9.6, loss: 0.25\n",
            "    x: 5.0, y: 10.9, y_hat: 11.8, loss: 0.81\n",
            "    Loss = 0.7368\n",
            "\n",
            "(w=2.2, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.6, loss: 0.02\n",
            "    x: 2.0, y: 5.0, y_hat: 5.4, loss: 0.16\n",
            "    x: 3.5, y: 7.9, y_hat: 8.7, loss: 0.64\n",
            "    x: 4.0, y: 9.1, y_hat: 9.8, loss: 0.49\n",
            "    x: 5.0, y: 10.9, y_hat: 12.0, loss: 1.21\n",
            "    Loss = 1.2648\n",
            "\n",
            "(w=2.2, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.8, loss: 0.12\n",
            "    x: 2.0, y: 5.0, y_hat: 5.6, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 8.9, loss: 1.00\n",
            "    x: 4.0, y: 9.1, y_hat: 10.0, loss: 0.81\n",
            "    x: 5.0, y: 10.9, y_hat: 12.2, loss: 1.69\n",
            "    Loss = 2.0328\n",
            "\n",
            "(w=2.2, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 4.0, loss: 0.29\n",
            "    x: 2.0, y: 5.0, y_hat: 5.8, loss: 0.64\n",
            "    x: 3.5, y: 7.9, y_hat: 9.1, loss: 1.44\n",
            "    x: 4.0, y: 9.1, y_hat: 10.2, loss: 1.21\n",
            "    x: 5.0, y: 10.9, y_hat: 12.4, loss: 2.25\n",
            "    Loss = 3.0408\n",
            "\n",
            "(w=2.2, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 4.2, loss: 0.55\n",
            "    x: 2.0, y: 5.0, y_hat: 6.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 9.3, loss: 1.96\n",
            "    x: 4.0, y: 9.1, y_hat: 10.4, loss: 1.69\n",
            "    x: 5.0, y: 10.9, y_hat: 12.6, loss: 2.89\n",
            "    Loss = 4.2888\n",
            "\n",
            "(w=2.2, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 4.4, loss: 0.88\n",
            "    x: 2.0, y: 5.0, y_hat: 6.2, loss: 1.44\n",
            "    x: 3.5, y: 7.9, y_hat: 9.5, loss: 2.56\n",
            "    x: 4.0, y: 9.1, y_hat: 10.6, loss: 2.25\n",
            "    x: 5.0, y: 10.9, y_hat: 12.8, loss: 3.61\n",
            "    Loss = 5.7768\n",
            "\n",
            "(w=2.2, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 4.6, loss: 1.30\n",
            "    x: 2.0, y: 5.0, y_hat: 6.4, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 9.7, loss: 3.24\n",
            "    x: 4.0, y: 9.1, y_hat: 10.8, loss: 2.89\n",
            "    x: 5.0, y: 10.9, y_hat: 13.0, loss: 4.41\n",
            "    Loss = 7.5048\n",
            "\n",
            "(w=2.4, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 2.9, loss: 0.38\n",
            "    x: 2.0, y: 5.0, y_hat: 4.8, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 8.4, loss: 0.25\n",
            "    x: 4.0, y: 9.1, y_hat: 9.6, loss: 0.25\n",
            "    x: 5.0, y: 10.9, y_hat: 12.0, loss: 1.21\n",
            "    Loss = 1.3122\n",
            "\n",
            "(w=2.4, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 3.1, loss: 0.18\n",
            "    x: 2.0, y: 5.0, y_hat: 5.0, loss: 0.00\n",
            "    x: 3.5, y: 7.9, y_hat: 8.6, loss: 0.49\n",
            "    x: 4.0, y: 9.1, y_hat: 9.8, loss: 0.49\n",
            "    x: 5.0, y: 10.9, y_hat: 12.2, loss: 1.69\n",
            "    Loss = 1.5482\n",
            "\n",
            "(w=2.4, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.3, loss: 0.05\n",
            "    x: 2.0, y: 5.0, y_hat: 5.2, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 8.8, loss: 0.81\n",
            "    x: 4.0, y: 9.1, y_hat: 10.0, loss: 0.81\n",
            "    x: 5.0, y: 10.9, y_hat: 12.4, loss: 2.25\n",
            "    Loss = 2.0242\n",
            "\n",
            "(w=2.4, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.5, loss: 0.00\n",
            "    x: 2.0, y: 5.0, y_hat: 5.4, loss: 0.16\n",
            "    x: 3.5, y: 7.9, y_hat: 9.0, loss: 1.21\n",
            "    x: 4.0, y: 9.1, y_hat: 10.2, loss: 1.21\n",
            "    x: 5.0, y: 10.9, y_hat: 12.6, loss: 2.89\n",
            "    Loss = 2.7402\n",
            "\n",
            "(w=2.4, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.7, loss: 0.03\n",
            "    x: 2.0, y: 5.0, y_hat: 5.6, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 9.2, loss: 1.69\n",
            "    x: 4.0, y: 9.1, y_hat: 10.4, loss: 1.69\n",
            "    x: 5.0, y: 10.9, y_hat: 12.8, loss: 3.61\n",
            "    Loss = 3.6962\n",
            "\n",
            "(w=2.4, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.9, loss: 0.14\n",
            "    x: 2.0, y: 5.0, y_hat: 5.8, loss: 0.64\n",
            "    x: 3.5, y: 7.9, y_hat: 9.4, loss: 2.25\n",
            "    x: 4.0, y: 9.1, y_hat: 10.6, loss: 2.25\n",
            "    x: 5.0, y: 10.9, y_hat: 13.0, loss: 4.41\n",
            "    Loss = 4.8922\n",
            "\n",
            "(w=2.4, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 4.1, loss: 0.34\n",
            "    x: 2.0, y: 5.0, y_hat: 6.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 9.6, loss: 2.89\n",
            "    x: 4.0, y: 9.1, y_hat: 10.8, loss: 2.89\n",
            "    x: 5.0, y: 10.9, y_hat: 13.2, loss: 5.29\n",
            "    Loss = 6.3282\n",
            "\n",
            "(w=2.4, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 4.3, loss: 0.61\n",
            "    x: 2.0, y: 5.0, y_hat: 6.2, loss: 1.44\n",
            "    x: 3.5, y: 7.9, y_hat: 9.8, loss: 3.61\n",
            "    x: 4.0, y: 9.1, y_hat: 11.0, loss: 3.61\n",
            "    x: 5.0, y: 10.9, y_hat: 13.4, loss: 6.25\n",
            "    Loss = 8.0042\n",
            "\n",
            "(w=2.4, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 4.5, loss: 0.96\n",
            "    x: 2.0, y: 5.0, y_hat: 6.4, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 10.0, loss: 4.41\n",
            "    x: 4.0, y: 9.1, y_hat: 11.2, loss: 4.41\n",
            "    x: 5.0, y: 10.9, y_hat: 13.6, loss: 7.29\n",
            "    Loss = 9.9202\n",
            "\n",
            "(w=2.4, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 4.7, loss: 1.39\n",
            "    x: 2.0, y: 5.0, y_hat: 6.6, loss: 2.56\n",
            "    x: 3.5, y: 7.9, y_hat: 10.2, loss: 5.29\n",
            "    x: 4.0, y: 9.1, y_hat: 11.4, loss: 5.29\n",
            "    x: 5.0, y: 10.9, y_hat: 13.8, loss: 8.41\n",
            "    Loss = 12.0762\n",
            "\n",
            "(w=2.4, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 4.9, loss: 1.90\n",
            "    x: 2.0, y: 5.0, y_hat: 6.8, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 10.4, loss: 6.25\n",
            "    x: 4.0, y: 9.1, y_hat: 11.6, loss: 6.25\n",
            "    x: 5.0, y: 10.9, y_hat: 14.0, loss: 9.61\n",
            "    Loss = 14.4722\n",
            "\n",
            "(w=2.6, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 3.1, loss: 0.14\n",
            "    x: 2.0, y: 5.0, y_hat: 5.2, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 9.1, loss: 1.44\n",
            "    x: 4.0, y: 9.1, y_hat: 10.4, loss: 1.69\n",
            "    x: 5.0, y: 10.9, y_hat: 13.0, loss: 4.41\n",
            "    Loss = 3.9872\n",
            "\n",
            "(w=2.6, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.3, loss: 0.03\n",
            "    x: 2.0, y: 5.0, y_hat: 5.4, loss: 0.16\n",
            "    x: 3.5, y: 7.9, y_hat: 9.3, loss: 1.96\n",
            "    x: 4.0, y: 9.1, y_hat: 10.6, loss: 2.25\n",
            "    x: 5.0, y: 10.9, y_hat: 13.2, loss: 5.29\n",
            "    Loss = 4.8912\n",
            "\n",
            "(w=2.6, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.5, loss: 0.00\n",
            "    x: 2.0, y: 5.0, y_hat: 5.6, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 9.5, loss: 2.56\n",
            "    x: 4.0, y: 9.1, y_hat: 10.8, loss: 2.89\n",
            "    x: 5.0, y: 10.9, y_hat: 13.4, loss: 6.25\n",
            "    Loss = 6.0352\n",
            "\n",
            "(w=2.6, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.7, loss: 0.05\n",
            "    x: 2.0, y: 5.0, y_hat: 5.8, loss: 0.64\n",
            "    x: 3.5, y: 7.9, y_hat: 9.7, loss: 3.24\n",
            "    x: 4.0, y: 9.1, y_hat: 11.0, loss: 3.61\n",
            "    x: 5.0, y: 10.9, y_hat: 13.6, loss: 7.29\n",
            "    Loss = 7.4192\n",
            "\n",
            "(w=2.6, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.9, loss: 0.18\n",
            "    x: 2.0, y: 5.0, y_hat: 6.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 9.9, loss: 4.00\n",
            "    x: 4.0, y: 9.1, y_hat: 11.2, loss: 4.41\n",
            "    x: 5.0, y: 10.9, y_hat: 13.8, loss: 8.41\n",
            "    Loss = 9.0432\n",
            "\n",
            "(w=2.6, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 4.1, loss: 0.38\n",
            "    x: 2.0, y: 5.0, y_hat: 6.2, loss: 1.44\n",
            "    x: 3.5, y: 7.9, y_hat: 10.1, loss: 4.84\n",
            "    x: 4.0, y: 9.1, y_hat: 11.4, loss: 5.29\n",
            "    x: 5.0, y: 10.9, y_hat: 14.0, loss: 9.61\n",
            "    Loss = 10.9072\n",
            "\n",
            "(w=2.6, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 4.3, loss: 0.67\n",
            "    x: 2.0, y: 5.0, y_hat: 6.4, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 10.3, loss: 5.76\n",
            "    x: 4.0, y: 9.1, y_hat: 11.6, loss: 6.25\n",
            "    x: 5.0, y: 10.9, y_hat: 14.2, loss: 10.89\n",
            "    Loss = 13.0112\n",
            "\n",
            "(w=2.6, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 4.5, loss: 1.04\n",
            "    x: 2.0, y: 5.0, y_hat: 6.6, loss: 2.56\n",
            "    x: 3.5, y: 7.9, y_hat: 10.5, loss: 6.76\n",
            "    x: 4.0, y: 9.1, y_hat: 11.8, loss: 7.29\n",
            "    x: 5.0, y: 10.9, y_hat: 14.4, loss: 12.25\n",
            "    Loss = 15.3552\n",
            "\n",
            "(w=2.6, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 4.7, loss: 1.49\n",
            "    x: 2.0, y: 5.0, y_hat: 6.8, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 10.7, loss: 7.84\n",
            "    x: 4.0, y: 9.1, y_hat: 12.0, loss: 8.41\n",
            "    x: 5.0, y: 10.9, y_hat: 14.6, loss: 13.69\n",
            "    Loss = 17.9392\n",
            "\n",
            "(w=2.6, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 4.9, loss: 2.02\n",
            "    x: 2.0, y: 5.0, y_hat: 7.0, loss: 4.00\n",
            "    x: 3.5, y: 7.9, y_hat: 10.9, loss: 9.00\n",
            "    x: 4.0, y: 9.1, y_hat: 12.2, loss: 9.61\n",
            "    x: 5.0, y: 10.9, y_hat: 14.8, loss: 15.21\n",
            "    Loss = 20.7632\n",
            "\n",
            "(w=2.6, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 5.1, loss: 2.62\n",
            "    x: 2.0, y: 5.0, y_hat: 7.2, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 11.1, loss: 10.24\n",
            "    x: 4.0, y: 9.1, y_hat: 12.4, loss: 10.89\n",
            "    x: 5.0, y: 10.9, y_hat: 15.0, loss: 16.81\n",
            "    Loss = 23.8272\n",
            "\n",
            "(w=2.8, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.4, loss: 0.02\n",
            "    x: 2.0, y: 5.0, y_hat: 5.6, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 9.8, loss: 3.61\n",
            "    x: 4.0, y: 9.1, y_hat: 11.2, loss: 4.41\n",
            "    x: 5.0, y: 10.9, y_hat: 14.0, loss: 9.61\n",
            "    Loss = 9.0498\n",
            "\n",
            "(w=2.8, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.6, loss: 0.00\n",
            "    x: 2.0, y: 5.0, y_hat: 5.8, loss: 0.64\n",
            "    x: 3.5, y: 7.9, y_hat: 10.0, loss: 4.41\n",
            "    x: 4.0, y: 9.1, y_hat: 11.4, loss: 5.29\n",
            "    x: 5.0, y: 10.9, y_hat: 14.2, loss: 10.89\n",
            "    Loss = 10.6218\n",
            "\n",
            "(w=2.8, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.8, loss: 0.07\n",
            "    x: 2.0, y: 5.0, y_hat: 6.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 10.2, loss: 5.29\n",
            "    x: 4.0, y: 9.1, y_hat: 11.6, loss: 6.25\n",
            "    x: 5.0, y: 10.9, y_hat: 14.4, loss: 12.25\n",
            "    Loss = 12.4338\n",
            "\n",
            "(w=2.8, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 4.0, loss: 0.21\n",
            "    x: 2.0, y: 5.0, y_hat: 6.2, loss: 1.44\n",
            "    x: 3.5, y: 7.9, y_hat: 10.4, loss: 6.25\n",
            "    x: 4.0, y: 9.1, y_hat: 11.8, loss: 7.29\n",
            "    x: 5.0, y: 10.9, y_hat: 14.6, loss: 13.69\n",
            "    Loss = 14.4858\n",
            "\n",
            "(w=2.8, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 4.2, loss: 0.44\n",
            "    x: 2.0, y: 5.0, y_hat: 6.4, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 10.6, loss: 7.29\n",
            "    x: 4.0, y: 9.1, y_hat: 12.0, loss: 8.41\n",
            "    x: 5.0, y: 10.9, y_hat: 14.8, loss: 15.21\n",
            "    Loss = 16.7778\n",
            "\n",
            "(w=2.8, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 4.4, loss: 0.74\n",
            "    x: 2.0, y: 5.0, y_hat: 6.6, loss: 2.56\n",
            "    x: 3.5, y: 7.9, y_hat: 10.8, loss: 8.41\n",
            "    x: 4.0, y: 9.1, y_hat: 12.2, loss: 9.61\n",
            "    x: 5.0, y: 10.9, y_hat: 15.0, loss: 16.81\n",
            "    Loss = 19.3098\n",
            "\n",
            "(w=2.8, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 4.6, loss: 1.12\n",
            "    x: 2.0, y: 5.0, y_hat: 6.8, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 11.0, loss: 9.61\n",
            "    x: 4.0, y: 9.1, y_hat: 12.4, loss: 10.89\n",
            "    x: 5.0, y: 10.9, y_hat: 15.2, loss: 18.49\n",
            "    Loss = 22.0818\n",
            "\n",
            "(w=2.8, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 4.8, loss: 1.59\n",
            "    x: 2.0, y: 5.0, y_hat: 7.0, loss: 4.00\n",
            "    x: 3.5, y: 7.9, y_hat: 11.2, loss: 10.89\n",
            "    x: 4.0, y: 9.1, y_hat: 12.6, loss: 12.25\n",
            "    x: 5.0, y: 10.9, y_hat: 15.4, loss: 20.25\n",
            "    Loss = 25.0938\n",
            "\n",
            "(w=2.8, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 5.0, loss: 2.13\n",
            "    x: 2.0, y: 5.0, y_hat: 7.2, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 11.4, loss: 12.25\n",
            "    x: 4.0, y: 9.1, y_hat: 12.8, loss: 13.69\n",
            "    x: 5.0, y: 10.9, y_hat: 15.6, loss: 22.09\n",
            "    Loss = 28.3458\n",
            "\n",
            "(w=2.8, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 5.2, loss: 2.76\n",
            "    x: 2.0, y: 5.0, y_hat: 7.4, loss: 5.76\n",
            "    x: 3.5, y: 7.9, y_hat: 11.6, loss: 13.69\n",
            "    x: 4.0, y: 9.1, y_hat: 13.0, loss: 15.21\n",
            "    x: 5.0, y: 10.9, y_hat: 15.8, loss: 24.01\n",
            "    Loss = 31.8378\n",
            "\n",
            "(w=2.8, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.8, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 5.4, loss: 3.46\n",
            "    x: 2.0, y: 5.0, y_hat: 7.6, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 11.8, loss: 15.21\n",
            "    x: 4.0, y: 9.1, y_hat: 13.2, loss: 16.81\n",
            "    x: 5.0, y: 10.9, y_hat: 16.0, loss: 26.01\n",
            "    Loss = 35.5698\n",
            "\n",
            "(w=3.0, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.6, loss: 0.01\n",
            "    x: 2.0, y: 5.0, y_hat: 6.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 10.5, loss: 6.76\n",
            "    x: 4.0, y: 9.1, y_hat: 12.0, loss: 8.41\n",
            "    x: 5.0, y: 10.9, y_hat: 15.0, loss: 16.81\n",
            "    Loss = 16.5000\n",
            "\n",
            "(w=3.0, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.8, loss: 0.09\n",
            "    x: 2.0, y: 5.0, y_hat: 6.2, loss: 1.44\n",
            "    x: 3.5, y: 7.9, y_hat: 10.7, loss: 7.84\n",
            "    x: 4.0, y: 9.1, y_hat: 12.2, loss: 9.61\n",
            "    x: 5.0, y: 10.9, y_hat: 15.2, loss: 18.49\n",
            "    Loss = 18.7400\n",
            "\n",
            "(w=3.0, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 4.0, loss: 0.25\n",
            "    x: 2.0, y: 5.0, y_hat: 6.4, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 10.9, loss: 9.00\n",
            "    x: 4.0, y: 9.1, y_hat: 12.4, loss: 10.89\n",
            "    x: 5.0, y: 10.9, y_hat: 15.4, loss: 20.25\n",
            "    Loss = 21.2200\n",
            "\n",
            "(w=3.0, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 4.2, loss: 0.49\n",
            "    x: 2.0, y: 5.0, y_hat: 6.6, loss: 2.56\n",
            "    x: 3.5, y: 7.9, y_hat: 11.1, loss: 10.24\n",
            "    x: 4.0, y: 9.1, y_hat: 12.6, loss: 12.25\n",
            "    x: 5.0, y: 10.9, y_hat: 15.6, loss: 22.09\n",
            "    Loss = 23.9400\n",
            "\n",
            "(w=3.0, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 4.4, loss: 0.81\n",
            "    x: 2.0, y: 5.0, y_hat: 6.8, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 11.3, loss: 11.56\n",
            "    x: 4.0, y: 9.1, y_hat: 12.8, loss: 13.69\n",
            "    x: 5.0, y: 10.9, y_hat: 15.8, loss: 24.01\n",
            "    Loss = 26.9000\n",
            "\n",
            "(w=3.0, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 4.6, loss: 1.21\n",
            "    x: 2.0, y: 5.0, y_hat: 7.0, loss: 4.00\n",
            "    x: 3.5, y: 7.9, y_hat: 11.5, loss: 12.96\n",
            "    x: 4.0, y: 9.1, y_hat: 13.0, loss: 15.21\n",
            "    x: 5.0, y: 10.9, y_hat: 16.0, loss: 26.01\n",
            "    Loss = 30.1000\n",
            "\n",
            "(w=3.0, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 4.8, loss: 1.69\n",
            "    x: 2.0, y: 5.0, y_hat: 7.2, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 11.7, loss: 14.44\n",
            "    x: 4.0, y: 9.1, y_hat: 13.2, loss: 16.81\n",
            "    x: 5.0, y: 10.9, y_hat: 16.2, loss: 28.09\n",
            "    Loss = 33.5400\n",
            "\n",
            "(w=3.0, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 5.0, loss: 2.25\n",
            "    x: 2.0, y: 5.0, y_hat: 7.4, loss: 5.76\n",
            "    x: 3.5, y: 7.9, y_hat: 11.9, loss: 16.00\n",
            "    x: 4.0, y: 9.1, y_hat: 13.4, loss: 18.49\n",
            "    x: 5.0, y: 10.9, y_hat: 16.4, loss: 30.25\n",
            "    Loss = 37.2200\n",
            "\n",
            "(w=3.0, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 5.2, loss: 2.89\n",
            "    x: 2.0, y: 5.0, y_hat: 7.6, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 12.1, loss: 17.64\n",
            "    x: 4.0, y: 9.1, y_hat: 13.6, loss: 20.25\n",
            "    x: 5.0, y: 10.9, y_hat: 16.6, loss: 32.49\n",
            "    Loss = 41.1400\n",
            "\n",
            "(w=3.0, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.8, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 5.4, loss: 3.61\n",
            "    x: 2.0, y: 5.0, y_hat: 7.8, loss: 7.84\n",
            "    x: 3.5, y: 7.9, y_hat: 12.3, loss: 19.36\n",
            "    x: 4.0, y: 9.1, y_hat: 13.8, loss: 22.09\n",
            "    x: 5.0, y: 10.9, y_hat: 16.8, loss: 34.81\n",
            "    Loss = 45.3000\n",
            "\n",
            "(w=3.0, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.0, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 5.6, loss: 4.41\n",
            "    x: 2.0, y: 5.0, y_hat: 8.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 12.5, loss: 21.16\n",
            "    x: 4.0, y: 9.1, y_hat: 14.0, loss: 24.01\n",
            "    x: 5.0, y: 10.9, y_hat: 17.0, loss: 37.21\n",
            "    Loss = 49.7000\n",
            "\n",
            "(w=3.2, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.8, loss: 0.12\n",
            "    x: 2.0, y: 5.0, y_hat: 6.4, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 11.2, loss: 10.89\n",
            "    x: 4.0, y: 9.1, y_hat: 12.8, loss: 13.69\n",
            "    x: 5.0, y: 10.9, y_hat: 16.0, loss: 26.01\n",
            "    Loss = 26.3378\n",
            "\n",
            "(w=3.2, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 4.0, loss: 0.29\n",
            "    x: 2.0, y: 5.0, y_hat: 6.6, loss: 2.56\n",
            "    x: 3.5, y: 7.9, y_hat: 11.4, loss: 12.25\n",
            "    x: 4.0, y: 9.1, y_hat: 13.0, loss: 15.21\n",
            "    x: 5.0, y: 10.9, y_hat: 16.2, loss: 28.09\n",
            "    Loss = 29.2458\n",
            "\n",
            "(w=3.2, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 4.2, loss: 0.55\n",
            "    x: 2.0, y: 5.0, y_hat: 6.8, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 11.6, loss: 13.69\n",
            "    x: 4.0, y: 9.1, y_hat: 13.2, loss: 16.81\n",
            "    x: 5.0, y: 10.9, y_hat: 16.4, loss: 30.25\n",
            "    Loss = 32.3938\n",
            "\n",
            "(w=3.2, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 4.4, loss: 0.88\n",
            "    x: 2.0, y: 5.0, y_hat: 7.0, loss: 4.00\n",
            "    x: 3.5, y: 7.9, y_hat: 11.8, loss: 15.21\n",
            "    x: 4.0, y: 9.1, y_hat: 13.4, loss: 18.49\n",
            "    x: 5.0, y: 10.9, y_hat: 16.6, loss: 32.49\n",
            "    Loss = 35.7818\n",
            "\n",
            "(w=3.2, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 4.6, loss: 1.30\n",
            "    x: 2.0, y: 5.0, y_hat: 7.2, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 12.0, loss: 16.81\n",
            "    x: 4.0, y: 9.1, y_hat: 13.6, loss: 20.25\n",
            "    x: 5.0, y: 10.9, y_hat: 16.8, loss: 34.81\n",
            "    Loss = 39.4098\n",
            "\n",
            "(w=3.2, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 4.8, loss: 1.80\n",
            "    x: 2.0, y: 5.0, y_hat: 7.4, loss: 5.76\n",
            "    x: 3.5, y: 7.9, y_hat: 12.2, loss: 18.49\n",
            "    x: 4.0, y: 9.1, y_hat: 13.8, loss: 22.09\n",
            "    x: 5.0, y: 10.9, y_hat: 17.0, loss: 37.21\n",
            "    Loss = 43.2778\n",
            "\n",
            "(w=3.2, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 5.0, loss: 2.37\n",
            "    x: 2.0, y: 5.0, y_hat: 7.6, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 12.4, loss: 20.25\n",
            "    x: 4.0, y: 9.1, y_hat: 14.0, loss: 24.01\n",
            "    x: 5.0, y: 10.9, y_hat: 17.2, loss: 39.69\n",
            "    Loss = 47.3858\n",
            "\n",
            "(w=3.2, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 5.2, loss: 3.03\n",
            "    x: 2.0, y: 5.0, y_hat: 7.8, loss: 7.84\n",
            "    x: 3.5, y: 7.9, y_hat: 12.6, loss: 22.09\n",
            "    x: 4.0, y: 9.1, y_hat: 14.2, loss: 26.01\n",
            "    x: 5.0, y: 10.9, y_hat: 17.4, loss: 42.25\n",
            "    Loss = 51.7338\n",
            "\n",
            "(w=3.2, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.8, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 5.4, loss: 3.76\n",
            "    x: 2.0, y: 5.0, y_hat: 8.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 12.8, loss: 24.01\n",
            "    x: 4.0, y: 9.1, y_hat: 14.4, loss: 28.09\n",
            "    x: 5.0, y: 10.9, y_hat: 17.6, loss: 44.89\n",
            "    Loss = 56.3218\n",
            "\n",
            "(w=3.2, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.0, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 5.6, loss: 4.58\n",
            "    x: 2.0, y: 5.0, y_hat: 8.2, loss: 10.24\n",
            "    x: 3.5, y: 7.9, y_hat: 13.0, loss: 26.01\n",
            "    x: 4.0, y: 9.1, y_hat: 14.6, loss: 30.25\n",
            "    x: 5.0, y: 10.9, y_hat: 17.8, loss: 47.61\n",
            "    Loss = 61.1498\n",
            "\n",
            "(w=3.2, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.2, loss: 4.41\n",
            "    x: 1.2, y: 3.5, y_hat: 5.8, loss: 5.48\n",
            "    x: 2.0, y: 5.0, y_hat: 8.4, loss: 11.56\n",
            "    x: 3.5, y: 7.9, y_hat: 13.2, loss: 28.09\n",
            "    x: 4.0, y: 9.1, y_hat: 14.8, loss: 32.49\n",
            "    x: 5.0, y: 10.9, y_hat: 18.0, loss: 50.41\n",
            "    Loss = 66.2178\n",
            "\n",
            "(w=3.4, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 4.1, loss: 0.34\n",
            "    x: 2.0, y: 5.0, y_hat: 6.8, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 11.9, loss: 16.00\n",
            "    x: 4.0, y: 9.1, y_hat: 13.6, loss: 20.25\n",
            "    x: 5.0, y: 10.9, y_hat: 17.0, loss: 37.21\n",
            "    Loss = 38.5632\n",
            "\n",
            "(w=3.4, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 4.3, loss: 0.61\n",
            "    x: 2.0, y: 5.0, y_hat: 7.0, loss: 4.00\n",
            "    x: 3.5, y: 7.9, y_hat: 12.1, loss: 17.64\n",
            "    x: 4.0, y: 9.1, y_hat: 13.8, loss: 22.09\n",
            "    x: 5.0, y: 10.9, y_hat: 17.2, loss: 39.69\n",
            "    Loss = 42.1392\n",
            "\n",
            "(w=3.4, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 4.5, loss: 0.96\n",
            "    x: 2.0, y: 5.0, y_hat: 7.2, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 12.3, loss: 19.36\n",
            "    x: 4.0, y: 9.1, y_hat: 14.0, loss: 24.01\n",
            "    x: 5.0, y: 10.9, y_hat: 17.4, loss: 42.25\n",
            "    Loss = 45.9552\n",
            "\n",
            "(w=3.4, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 4.7, loss: 1.39\n",
            "    x: 2.0, y: 5.0, y_hat: 7.4, loss: 5.76\n",
            "    x: 3.5, y: 7.9, y_hat: 12.5, loss: 21.16\n",
            "    x: 4.0, y: 9.1, y_hat: 14.2, loss: 26.01\n",
            "    x: 5.0, y: 10.9, y_hat: 17.6, loss: 44.89\n",
            "    Loss = 50.0112\n",
            "\n",
            "(w=3.4, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 4.9, loss: 1.90\n",
            "    x: 2.0, y: 5.0, y_hat: 7.6, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 12.7, loss: 23.04\n",
            "    x: 4.0, y: 9.1, y_hat: 14.4, loss: 28.09\n",
            "    x: 5.0, y: 10.9, y_hat: 17.8, loss: 47.61\n",
            "    Loss = 54.3072\n",
            "\n",
            "(w=3.4, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 5.1, loss: 2.50\n",
            "    x: 2.0, y: 5.0, y_hat: 7.8, loss: 7.84\n",
            "    x: 3.5, y: 7.9, y_hat: 12.9, loss: 25.00\n",
            "    x: 4.0, y: 9.1, y_hat: 14.6, loss: 30.25\n",
            "    x: 5.0, y: 10.9, y_hat: 18.0, loss: 50.41\n",
            "    Loss = 58.8432\n",
            "\n",
            "(w=3.4, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 5.3, loss: 3.17\n",
            "    x: 2.0, y: 5.0, y_hat: 8.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 13.1, loss: 27.04\n",
            "    x: 4.0, y: 9.1, y_hat: 14.8, loss: 32.49\n",
            "    x: 5.0, y: 10.9, y_hat: 18.2, loss: 53.29\n",
            "    Loss = 63.6192\n",
            "\n",
            "(w=3.4, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.8, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 5.5, loss: 3.92\n",
            "    x: 2.0, y: 5.0, y_hat: 8.2, loss: 10.24\n",
            "    x: 3.5, y: 7.9, y_hat: 13.3, loss: 29.16\n",
            "    x: 4.0, y: 9.1, y_hat: 15.0, loss: 34.81\n",
            "    x: 5.0, y: 10.9, y_hat: 18.4, loss: 56.25\n",
            "    Loss = 68.6352\n",
            "\n",
            "(w=3.4, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.0, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 5.7, loss: 4.75\n",
            "    x: 2.0, y: 5.0, y_hat: 8.4, loss: 11.56\n",
            "    x: 3.5, y: 7.9, y_hat: 13.5, loss: 31.36\n",
            "    x: 4.0, y: 9.1, y_hat: 15.2, loss: 37.21\n",
            "    x: 5.0, y: 10.9, y_hat: 18.6, loss: 59.29\n",
            "    Loss = 73.8912\n",
            "\n",
            "(w=3.4, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.2, loss: 4.41\n",
            "    x: 1.2, y: 3.5, y_hat: 5.9, loss: 5.66\n",
            "    x: 2.0, y: 5.0, y_hat: 8.6, loss: 12.96\n",
            "    x: 3.5, y: 7.9, y_hat: 13.7, loss: 33.64\n",
            "    x: 4.0, y: 9.1, y_hat: 15.4, loss: 39.69\n",
            "    x: 5.0, y: 10.9, y_hat: 18.8, loss: 62.41\n",
            "    Loss = 79.3872\n",
            "\n",
            "(w=3.4, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.4, loss: 5.29\n",
            "    x: 1.2, y: 3.5, y_hat: 6.1, loss: 6.66\n",
            "    x: 2.0, y: 5.0, y_hat: 8.8, loss: 14.44\n",
            "    x: 3.5, y: 7.9, y_hat: 13.9, loss: 36.00\n",
            "    x: 4.0, y: 9.1, y_hat: 15.6, loss: 42.25\n",
            "    x: 5.0, y: 10.9, y_hat: 19.0, loss: 65.61\n",
            "    Loss = 85.1232\n",
            "\n",
            "(w=3.6, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 4.3, loss: 0.67\n",
            "    x: 2.0, y: 5.0, y_hat: 7.2, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 12.6, loss: 22.09\n",
            "    x: 4.0, y: 9.1, y_hat: 14.4, loss: 28.09\n",
            "    x: 5.0, y: 10.9, y_hat: 18.0, loss: 50.41\n",
            "    Loss = 53.1762\n",
            "\n",
            "(w=3.6, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 4.5, loss: 1.04\n",
            "    x: 2.0, y: 5.0, y_hat: 7.4, loss: 5.76\n",
            "    x: 3.5, y: 7.9, y_hat: 12.8, loss: 24.01\n",
            "    x: 4.0, y: 9.1, y_hat: 14.6, loss: 30.25\n",
            "    x: 5.0, y: 10.9, y_hat: 18.2, loss: 53.29\n",
            "    Loss = 57.4202\n",
            "\n",
            "(w=3.6, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 4.7, loss: 1.49\n",
            "    x: 2.0, y: 5.0, y_hat: 7.6, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 13.0, loss: 26.01\n",
            "    x: 4.0, y: 9.1, y_hat: 14.8, loss: 32.49\n",
            "    x: 5.0, y: 10.9, y_hat: 18.4, loss: 56.25\n",
            "    Loss = 61.9042\n",
            "\n",
            "(w=3.6, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 4.9, loss: 2.02\n",
            "    x: 2.0, y: 5.0, y_hat: 7.8, loss: 7.84\n",
            "    x: 3.5, y: 7.9, y_hat: 13.2, loss: 28.09\n",
            "    x: 4.0, y: 9.1, y_hat: 15.0, loss: 34.81\n",
            "    x: 5.0, y: 10.9, y_hat: 18.6, loss: 59.29\n",
            "    Loss = 66.6282\n",
            "\n",
            "(w=3.6, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 5.1, loss: 2.62\n",
            "    x: 2.0, y: 5.0, y_hat: 8.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 13.4, loss: 30.25\n",
            "    x: 4.0, y: 9.1, y_hat: 15.2, loss: 37.21\n",
            "    x: 5.0, y: 10.9, y_hat: 18.8, loss: 62.41\n",
            "    Loss = 71.5922\n",
            "\n",
            "(w=3.6, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 5.3, loss: 3.31\n",
            "    x: 2.0, y: 5.0, y_hat: 8.2, loss: 10.24\n",
            "    x: 3.5, y: 7.9, y_hat: 13.6, loss: 32.49\n",
            "    x: 4.0, y: 9.1, y_hat: 15.4, loss: 39.69\n",
            "    x: 5.0, y: 10.9, y_hat: 19.0, loss: 65.61\n",
            "    Loss = 76.7962\n",
            "\n",
            "(w=3.6, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.8, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 5.5, loss: 4.08\n",
            "    x: 2.0, y: 5.0, y_hat: 8.4, loss: 11.56\n",
            "    x: 3.5, y: 7.9, y_hat: 13.8, loss: 34.81\n",
            "    x: 4.0, y: 9.1, y_hat: 15.6, loss: 42.25\n",
            "    x: 5.0, y: 10.9, y_hat: 19.2, loss: 68.89\n",
            "    Loss = 82.2402\n",
            "\n",
            "(w=3.6, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.0, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 5.7, loss: 4.93\n",
            "    x: 2.0, y: 5.0, y_hat: 8.6, loss: 12.96\n",
            "    x: 3.5, y: 7.9, y_hat: 14.0, loss: 37.21\n",
            "    x: 4.0, y: 9.1, y_hat: 15.8, loss: 44.89\n",
            "    x: 5.0, y: 10.9, y_hat: 19.4, loss: 72.25\n",
            "    Loss = 87.9242\n",
            "\n",
            "(w=3.6, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.2, loss: 4.41\n",
            "    x: 1.2, y: 3.5, y_hat: 5.9, loss: 5.86\n",
            "    x: 2.0, y: 5.0, y_hat: 8.8, loss: 14.44\n",
            "    x: 3.5, y: 7.9, y_hat: 14.2, loss: 39.69\n",
            "    x: 4.0, y: 9.1, y_hat: 16.0, loss: 47.61\n",
            "    x: 5.0, y: 10.9, y_hat: 19.6, loss: 75.69\n",
            "    Loss = 93.8482\n",
            "\n",
            "(w=3.6, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.4, loss: 5.29\n",
            "    x: 1.2, y: 3.5, y_hat: 6.1, loss: 6.86\n",
            "    x: 2.0, y: 5.0, y_hat: 9.0, loss: 16.00\n",
            "    x: 3.5, y: 7.9, y_hat: 14.4, loss: 42.25\n",
            "    x: 4.0, y: 9.1, y_hat: 16.2, loss: 50.41\n",
            "    x: 5.0, y: 10.9, y_hat: 19.8, loss: 79.21\n",
            "    Loss = 100.0122\n",
            "\n",
            "(w=3.6, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.6, loss: 6.25\n",
            "    x: 1.2, y: 3.5, y_hat: 6.3, loss: 7.95\n",
            "    x: 2.0, y: 5.0, y_hat: 9.2, loss: 17.64\n",
            "    x: 3.5, y: 7.9, y_hat: 14.6, loss: 44.89\n",
            "    x: 4.0, y: 9.1, y_hat: 16.4, loss: 53.29\n",
            "    x: 5.0, y: 10.9, y_hat: 20.0, loss: 82.81\n",
            "    Loss = 106.4162\n",
            "\n",
            "(w=3.8, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 4.6, loss: 1.12\n",
            "    x: 2.0, y: 5.0, y_hat: 7.6, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 13.3, loss: 29.16\n",
            "    x: 4.0, y: 9.1, y_hat: 15.2, loss: 37.21\n",
            "    x: 5.0, y: 10.9, y_hat: 19.0, loss: 65.61\n",
            "    Loss = 70.1768\n",
            "\n",
            "(w=3.8, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 4.8, loss: 1.59\n",
            "    x: 2.0, y: 5.0, y_hat: 7.8, loss: 7.84\n",
            "    x: 3.5, y: 7.9, y_hat: 13.5, loss: 31.36\n",
            "    x: 4.0, y: 9.1, y_hat: 15.4, loss: 39.69\n",
            "    x: 5.0, y: 10.9, y_hat: 19.2, loss: 68.89\n",
            "    Loss = 75.0888\n",
            "\n",
            "(w=3.8, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 5.0, loss: 2.13\n",
            "    x: 2.0, y: 5.0, y_hat: 8.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 13.7, loss: 33.64\n",
            "    x: 4.0, y: 9.1, y_hat: 15.6, loss: 42.25\n",
            "    x: 5.0, y: 10.9, y_hat: 19.4, loss: 72.25\n",
            "    Loss = 80.2408\n",
            "\n",
            "(w=3.8, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 5.2, loss: 2.76\n",
            "    x: 2.0, y: 5.0, y_hat: 8.2, loss: 10.24\n",
            "    x: 3.5, y: 7.9, y_hat: 13.9, loss: 36.00\n",
            "    x: 4.0, y: 9.1, y_hat: 15.8, loss: 44.89\n",
            "    x: 5.0, y: 10.9, y_hat: 19.6, loss: 75.69\n",
            "    Loss = 85.6328\n",
            "\n",
            "(w=3.8, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 5.4, loss: 3.46\n",
            "    x: 2.0, y: 5.0, y_hat: 8.4, loss: 11.56\n",
            "    x: 3.5, y: 7.9, y_hat: 14.1, loss: 38.44\n",
            "    x: 4.0, y: 9.1, y_hat: 16.0, loss: 47.61\n",
            "    x: 5.0, y: 10.9, y_hat: 19.8, loss: 79.21\n",
            "    Loss = 91.2648\n",
            "\n",
            "(w=3.8, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.8, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 5.6, loss: 4.24\n",
            "    x: 2.0, y: 5.0, y_hat: 8.6, loss: 12.96\n",
            "    x: 3.5, y: 7.9, y_hat: 14.3, loss: 40.96\n",
            "    x: 4.0, y: 9.1, y_hat: 16.2, loss: 50.41\n",
            "    x: 5.0, y: 10.9, y_hat: 20.0, loss: 82.81\n",
            "    Loss = 97.1368\n",
            "\n",
            "(w=3.8, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.0, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 5.8, loss: 5.11\n",
            "    x: 2.0, y: 5.0, y_hat: 8.8, loss: 14.44\n",
            "    x: 3.5, y: 7.9, y_hat: 14.5, loss: 43.56\n",
            "    x: 4.0, y: 9.1, y_hat: 16.4, loss: 53.29\n",
            "    x: 5.0, y: 10.9, y_hat: 20.2, loss: 86.49\n",
            "    Loss = 103.2488\n",
            "\n",
            "(w=3.8, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.2, loss: 4.41\n",
            "    x: 1.2, y: 3.5, y_hat: 6.0, loss: 6.05\n",
            "    x: 2.0, y: 5.0, y_hat: 9.0, loss: 16.00\n",
            "    x: 3.5, y: 7.9, y_hat: 14.7, loss: 46.24\n",
            "    x: 4.0, y: 9.1, y_hat: 16.6, loss: 56.25\n",
            "    x: 5.0, y: 10.9, y_hat: 20.4, loss: 90.25\n",
            "    Loss = 109.6008\n",
            "\n",
            "(w=3.8, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.4, loss: 5.29\n",
            "    x: 1.2, y: 3.5, y_hat: 6.2, loss: 7.08\n",
            "    x: 2.0, y: 5.0, y_hat: 9.2, loss: 17.64\n",
            "    x: 3.5, y: 7.9, y_hat: 14.9, loss: 49.00\n",
            "    x: 4.0, y: 9.1, y_hat: 16.8, loss: 59.29\n",
            "    x: 5.0, y: 10.9, y_hat: 20.6, loss: 94.09\n",
            "    Loss = 116.1928\n",
            "\n",
            "(w=3.8, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.6, loss: 6.25\n",
            "    x: 1.2, y: 3.5, y_hat: 6.4, loss: 8.18\n",
            "    x: 2.0, y: 5.0, y_hat: 9.4, loss: 19.36\n",
            "    x: 3.5, y: 7.9, y_hat: 15.1, loss: 51.84\n",
            "    x: 4.0, y: 9.1, y_hat: 17.0, loss: 62.41\n",
            "    x: 5.0, y: 10.9, y_hat: 20.8, loss: 98.01\n",
            "    Loss = 123.0248\n",
            "\n",
            "(w=3.8, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.8, loss: 7.29\n",
            "    x: 1.2, y: 3.5, y_hat: 6.6, loss: 9.36\n",
            "    x: 2.0, y: 5.0, y_hat: 9.6, loss: 21.16\n",
            "    x: 3.5, y: 7.9, y_hat: 15.3, loss: 54.76\n",
            "    x: 4.0, y: 9.1, y_hat: 17.2, loss: 65.61\n",
            "    x: 5.0, y: 10.9, y_hat: 21.0, loss: 102.01\n",
            "    Loss = 130.0968\n",
            "\n",
            "(w=4.0, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 4.8, loss: 1.69\n",
            "    x: 2.0, y: 5.0, y_hat: 8.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 14.0, loss: 37.21\n",
            "    x: 4.0, y: 9.1, y_hat: 16.0, loss: 47.61\n",
            "    x: 5.0, y: 10.9, y_hat: 20.0, loss: 82.81\n",
            "    Loss = 89.5650\n",
            "\n",
            "(w=4.0, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 5.0, loss: 2.25\n",
            "    x: 2.0, y: 5.0, y_hat: 8.2, loss: 10.24\n",
            "    x: 3.5, y: 7.9, y_hat: 14.2, loss: 39.69\n",
            "    x: 4.0, y: 9.1, y_hat: 16.2, loss: 50.41\n",
            "    x: 5.0, y: 10.9, y_hat: 20.2, loss: 86.49\n",
            "    Loss = 95.1450\n",
            "\n",
            "(w=4.0, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 5.2, loss: 2.89\n",
            "    x: 2.0, y: 5.0, y_hat: 8.4, loss: 11.56\n",
            "    x: 3.5, y: 7.9, y_hat: 14.4, loss: 42.25\n",
            "    x: 4.0, y: 9.1, y_hat: 16.4, loss: 53.29\n",
            "    x: 5.0, y: 10.9, y_hat: 20.4, loss: 90.25\n",
            "    Loss = 100.9650\n",
            "\n",
            "(w=4.0, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 5.4, loss: 3.61\n",
            "    x: 2.0, y: 5.0, y_hat: 8.6, loss: 12.96\n",
            "    x: 3.5, y: 7.9, y_hat: 14.6, loss: 44.89\n",
            "    x: 4.0, y: 9.1, y_hat: 16.6, loss: 56.25\n",
            "    x: 5.0, y: 10.9, y_hat: 20.6, loss: 94.09\n",
            "    Loss = 107.0250\n",
            "\n",
            "(w=4.0, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.8, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 5.6, loss: 4.41\n",
            "    x: 2.0, y: 5.0, y_hat: 8.8, loss: 14.44\n",
            "    x: 3.5, y: 7.9, y_hat: 14.8, loss: 47.61\n",
            "    x: 4.0, y: 9.1, y_hat: 16.8, loss: 59.29\n",
            "    x: 5.0, y: 10.9, y_hat: 20.8, loss: 98.01\n",
            "    Loss = 113.3250\n",
            "\n",
            "(w=4.0, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.0, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 5.8, loss: 5.29\n",
            "    x: 2.0, y: 5.0, y_hat: 9.0, loss: 16.00\n",
            "    x: 3.5, y: 7.9, y_hat: 15.0, loss: 50.41\n",
            "    x: 4.0, y: 9.1, y_hat: 17.0, loss: 62.41\n",
            "    x: 5.0, y: 10.9, y_hat: 21.0, loss: 102.01\n",
            "    Loss = 119.8650\n",
            "\n",
            "(w=4.0, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.2, loss: 4.41\n",
            "    x: 1.2, y: 3.5, y_hat: 6.0, loss: 6.25\n",
            "    x: 2.0, y: 5.0, y_hat: 9.2, loss: 17.64\n",
            "    x: 3.5, y: 7.9, y_hat: 15.2, loss: 53.29\n",
            "    x: 4.0, y: 9.1, y_hat: 17.2, loss: 65.61\n",
            "    x: 5.0, y: 10.9, y_hat: 21.2, loss: 106.09\n",
            "    Loss = 126.6450\n",
            "\n",
            "(w=4.0, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.4, loss: 5.29\n",
            "    x: 1.2, y: 3.5, y_hat: 6.2, loss: 7.29\n",
            "    x: 2.0, y: 5.0, y_hat: 9.4, loss: 19.36\n",
            "    x: 3.5, y: 7.9, y_hat: 15.4, loss: 56.25\n",
            "    x: 4.0, y: 9.1, y_hat: 17.4, loss: 68.89\n",
            "    x: 5.0, y: 10.9, y_hat: 21.4, loss: 110.25\n",
            "    Loss = 133.6650\n",
            "\n",
            "(w=4.0, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.6, loss: 6.25\n",
            "    x: 1.2, y: 3.5, y_hat: 6.4, loss: 8.41\n",
            "    x: 2.0, y: 5.0, y_hat: 9.6, loss: 21.16\n",
            "    x: 3.5, y: 7.9, y_hat: 15.6, loss: 59.29\n",
            "    x: 4.0, y: 9.1, y_hat: 17.6, loss: 72.25\n",
            "    x: 5.0, y: 10.9, y_hat: 21.6, loss: 114.49\n",
            "    Loss = 140.9250\n",
            "\n",
            "(w=4.0, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.8, loss: 7.29\n",
            "    x: 1.2, y: 3.5, y_hat: 6.6, loss: 9.61\n",
            "    x: 2.0, y: 5.0, y_hat: 9.8, loss: 23.04\n",
            "    x: 3.5, y: 7.9, y_hat: 15.8, loss: 62.41\n",
            "    x: 4.0, y: 9.1, y_hat: 17.8, loss: 75.69\n",
            "    x: 5.0, y: 10.9, y_hat: 21.8, loss: 118.81\n",
            "    Loss = 148.4250\n",
            "\n",
            "(w=4.0, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 6.0, loss: 8.41\n",
            "    x: 1.2, y: 3.5, y_hat: 6.8, loss: 10.89\n",
            "    x: 2.0, y: 5.0, y_hat: 10.0, loss: 25.00\n",
            "    x: 3.5, y: 7.9, y_hat: 16.0, loss: 65.61\n",
            "    x: 4.0, y: 9.1, y_hat: 18.0, loss: 79.21\n",
            "    x: 5.0, y: 10.9, y_hat: 22.0, loss: 123.21\n",
            "    Loss = 156.1650\n",
            "\n",
            "BEST:\n",
            "w=2.0, b=1.0, loss=0.0250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdvNc6WQOwhv"
      },
      "source": [
        "Plotting the loss values as a surface graph gives you a picture of the \"optimisation landscape\" for the parameter values. The loss is minimum at $w=2$, $b=1$ (it might be hard to see this clearly from the 3D diagram, but you can trust the numbers)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVruAU86OwIA",
        "outputId": "9a11b8e8-3633-4980-947a-7c88fedab5f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "fig = plt.figure()\n",
        "\n",
        "# enable 3D\n",
        "ax = fig.gca(projection='3d')\n",
        "\n",
        " # generate combinations of weights and biases\n",
        "(w_list, b_list) = np.meshgrid(weights, biases)\n",
        "\n",
        "# plot loss across weights and bias values\n",
        "surf = ax.plot_surface(w_list.T, b_list.T, loss_matrix,\n",
        "                       linewidth=0, antialiased=False)\n",
        "\n",
        "ax.set_xlabel('w')\n",
        "ax.set_ylabel('b')\n",
        "ax.set_zlabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nWYSrkBcWzs"
      },
      "source": [
        "### Optimisation with derivatives\n",
        "\n",
        "Obviously, the brute force search method above is really computationally expensive (very slow!), especially when there are more parameters and more values to search. A more efficient approach is to start from a random set of parameter values, and then try to \"move down\" the loss graph to the point with a minimum loss. So, in the plot above, you can start at some random $w$ and $b$, and try to move towards the minimum point.\n",
        "\n",
        "The problem is: from where you are, you actually do not know where the minimum point is. You only know that there *is* a minimum point. The question is: how do you know towards which direction you should move (and how far) so that you can get to the minimum point as fast as possible? The answer is downwards (obviously!) and where the slope is steepest. \n",
        "\n",
        "Fortunately, calculus saves us from guessing, and provides us a way to compute the direction of the slope at any point. This is called the *derivative* (or gradient). Intuitively, the derivative $\\frac{\\partial L}{\\partial w}$ tells us by how much $L$ changes when $w$ changes. Similarly, $\\frac{\\partial L}{\\partial b}$ tells us by how much $L$ changes when $b$ changes. So if we move in these directions, we hope that we can ultimately reach a minimum point.\n",
        "\n",
        "If you are well-versed with calculus, you can compute $\\frac{\\partial L}{\\partial w}$ and $\\frac{\\partial L}{\\partial b}$ by hand. Otherwise, you can get help from a derivative calculator (e.g. https://www.derivative-calculator.net)\n",
        "\n",
        "As presented in the lectures, the partial derivatives of the loss function with respect to $w$ and to $b$ are:\n",
        "\n",
        "$\\frac{\\partial L}{\\partial w} = \\sum_{i=1}^{N} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)x$ \n",
        "\n",
        "$\\frac{\\partial L}{\\partial b} = \\sum_{i=1}^{N} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)$ \n",
        "\n",
        "Thus, for a *single* point, the partial derivatives are:\n",
        "\n",
        "$\\frac{\\partial L^{(i)}}{\\partial w} = \\left(\\hat{y}^{(i)} - y^{(i)}\\right)x$ \n",
        "\n",
        "$\\frac{\\partial L^{(i)}}{\\partial b} = \\left(\\hat{y}^{(i)} - y^{(i)}\\right)$ \n",
        "\n",
        "Now, complete the `gradient()` method for `SimpleLinearRegression` below to compute the partial derivatives wrt $w$ and $b$ at a given point. To make life easier, just compute and return both $\\frac{\\partial L^{(i)}}{\\partial w}$ and $\\frac{\\partial L^{(i)}}{\\partial b}$ at the same time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBXmI71Ewc9e",
        "outputId": "043832e4-7abd-4737-e7f6-6ee72e2b5156",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def gradient(self, x, y):\n",
        "    \"\"\" Compute partial derivatives wrt w and b\n",
        "\n",
        "    Args:\n",
        "        x (float): input instance\n",
        "        y (float): ground truth output\n",
        "\n",
        "    Returns:\n",
        "        tuple: (float, float)\n",
        "            - the first element will be dL/dw\n",
        "            - the second element will be dL/db\n",
        "    \"\"\"\n",
        "    # TODO: Complete this\n",
        "\n",
        "    return ((self.forward(x) - y) * x, (self.forward(x) - y))\n",
        "    \n",
        "\n",
        "# A quick hack to bind this function as the SimpleLinearRegression.gradient() method\n",
        "SimpleLinearRegression.gradient = gradient\n",
        "\n",
        "\n",
        "## Quick test: This should return (6.0, 3.0)\n",
        "model = SimpleLinearRegression()\n",
        "model.w = 3\n",
        "model.b = 2\n",
        "x = 2.0\n",
        "y = 5.0\n",
        "(dLdw, dLdb) = model.gradient(x, y)\n",
        "print(dLdw) # should print 6.0\n",
        "print(dLdb) # should print 3.0"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.0\n",
            "3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBw1pRVN-heL"
      },
      "source": [
        "So, in this example, $\\frac{\\partial L}{\\partial w}=6.0$ suggests that when $w$ increases by a very tiny amount, $L$ will increase by 6.0 times that amount.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCNctFEY0yiT"
      },
      "source": [
        "#### Gradient descent\n",
        "\n",
        "Now that we have our gradients, let us try to optimise the parameters $w$ and $b$ of our model to minimise the loss. We will use the gradient descent algorithm for this, as discussed in the lectures. \n",
        "\n",
        "You may reimplement the code provided in the lectures, replacing some of the code by reusing the `forward()`, `loss()` and `gradient()` methods of `SimpleLinearRegression` that you implemented earlier. This will help make your code more modular and readable, and help to improve your understanding of gradient descent at a more abstract level.\n",
        "\n",
        "You should be able to obtain $w \\approx 2$ and $b \\approx 1$ by the end of training if you have implemented everything correctly. Also experiment with the learning rate and the number of epochs and observe the effects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLdqp4ESCpsR",
        "outputId": "65b6f11e-1f34-47fd-fdf8-fe82b43eaef8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = SimpleLinearRegression()\n",
        "\n",
        "learning_rate = 0.01\n",
        "n_epochs = 100\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    error = 0.0\n",
        "    grad_w = 0.0\n",
        "    grad_b = 0.0\n",
        "    for (x, y) in zip(x_train, y_train):\n",
        "        ### TODO: Complete this\n",
        "        ### 1. Compute the gradients for w and b for this example\n",
        "        dLdw, dLdb = model.gradient(x, y)\n",
        "\n",
        "        ### 2. Add the gradients to grad_w and grad_b\n",
        "        grad_w += dLdw\n",
        "        grad_b += dLdb\n",
        "\n",
        "        ### 3. Add the \"local\" loss to the global error (Loss) for analysis\n",
        "        error += model.loss(x, y)\n",
        "\n",
        "    # TODO: Update the weights using the (summed) gradients\n",
        "    model.w = model.w - (learning_rate * grad_w)\n",
        "    model.b = model.b - (learning_rate * grad_b)\n",
        "    \n",
        "    print(f\"Epoch: {epoch}\\t w: {model.w:.2f}\\t b: {model.b:.2f}\\t L: {error:.4f}\")\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\t w: 1.60\t b: 0.29\t L: 167.4422\n",
            "Epoch: 1\t w: 1.96\t b: 0.40\t L: 21.6991\n",
            "Epoch: 2\t w: 2.08\t b: 0.45\t L: 3.2782\n",
            "Epoch: 3\t w: 2.12\t b: 0.47\t L: 0.9390\n",
            "Epoch: 4\t w: 2.14\t b: 0.48\t L: 0.6314\n",
            "Epoch: 5\t w: 2.14\t b: 0.49\t L: 0.5806\n",
            "Epoch: 6\t w: 2.14\t b: 0.50\t L: 0.5625\n",
            "Epoch: 7\t w: 2.14\t b: 0.51\t L: 0.5489\n",
            "Epoch: 8\t w: 2.14\t b: 0.51\t L: 0.5361\n",
            "Epoch: 9\t w: 2.13\t b: 0.52\t L: 0.5237\n",
            "Epoch: 10\t w: 2.13\t b: 0.53\t L: 0.5116\n",
            "Epoch: 11\t w: 2.13\t b: 0.54\t L: 0.4998\n",
            "Epoch: 12\t w: 2.13\t b: 0.54\t L: 0.4883\n",
            "Epoch: 13\t w: 2.13\t b: 0.55\t L: 0.4771\n",
            "Epoch: 14\t w: 2.12\t b: 0.56\t L: 0.4661\n",
            "Epoch: 15\t w: 2.12\t b: 0.56\t L: 0.4554\n",
            "Epoch: 16\t w: 2.12\t b: 0.57\t L: 0.4450\n",
            "Epoch: 17\t w: 2.12\t b: 0.58\t L: 0.4348\n",
            "Epoch: 18\t w: 2.12\t b: 0.58\t L: 0.4249\n",
            "Epoch: 19\t w: 2.11\t b: 0.59\t L: 0.4152\n",
            "Epoch: 20\t w: 2.11\t b: 0.60\t L: 0.4058\n",
            "Epoch: 21\t w: 2.11\t b: 0.60\t L: 0.3966\n",
            "Epoch: 22\t w: 2.11\t b: 0.61\t L: 0.3876\n",
            "Epoch: 23\t w: 2.11\t b: 0.62\t L: 0.3788\n",
            "Epoch: 24\t w: 2.10\t b: 0.62\t L: 0.3703\n",
            "Epoch: 25\t w: 2.10\t b: 0.63\t L: 0.3619\n",
            "Epoch: 26\t w: 2.10\t b: 0.64\t L: 0.3538\n",
            "Epoch: 27\t w: 2.10\t b: 0.64\t L: 0.3458\n",
            "Epoch: 28\t w: 2.10\t b: 0.65\t L: 0.3381\n",
            "Epoch: 29\t w: 2.10\t b: 0.65\t L: 0.3305\n",
            "Epoch: 30\t w: 2.09\t b: 0.66\t L: 0.3232\n",
            "Epoch: 31\t w: 2.09\t b: 0.66\t L: 0.3160\n",
            "Epoch: 32\t w: 2.09\t b: 0.67\t L: 0.3089\n",
            "Epoch: 33\t w: 2.09\t b: 0.68\t L: 0.3021\n",
            "Epoch: 34\t w: 2.09\t b: 0.68\t L: 0.2954\n",
            "Epoch: 35\t w: 2.09\t b: 0.69\t L: 0.2889\n",
            "Epoch: 36\t w: 2.08\t b: 0.69\t L: 0.2825\n",
            "Epoch: 37\t w: 2.08\t b: 0.70\t L: 0.2763\n",
            "Epoch: 38\t w: 2.08\t b: 0.70\t L: 0.2703\n",
            "Epoch: 39\t w: 2.08\t b: 0.71\t L: 0.2644\n",
            "Epoch: 40\t w: 2.08\t b: 0.71\t L: 0.2586\n",
            "Epoch: 41\t w: 2.08\t b: 0.72\t L: 0.2530\n",
            "Epoch: 42\t w: 2.08\t b: 0.72\t L: 0.2475\n",
            "Epoch: 43\t w: 2.07\t b: 0.73\t L: 0.2422\n",
            "Epoch: 44\t w: 2.07\t b: 0.73\t L: 0.2370\n",
            "Epoch: 45\t w: 2.07\t b: 0.74\t L: 0.2319\n",
            "Epoch: 46\t w: 2.07\t b: 0.74\t L: 0.2269\n",
            "Epoch: 47\t w: 2.07\t b: 0.75\t L: 0.2221\n",
            "Epoch: 48\t w: 2.07\t b: 0.75\t L: 0.2174\n",
            "Epoch: 49\t w: 2.07\t b: 0.76\t L: 0.2128\n",
            "Epoch: 50\t w: 2.07\t b: 0.76\t L: 0.2083\n",
            "Epoch: 51\t w: 2.06\t b: 0.77\t L: 0.2039\n",
            "Epoch: 52\t w: 2.06\t b: 0.77\t L: 0.1996\n",
            "Epoch: 53\t w: 2.06\t b: 0.77\t L: 0.1954\n",
            "Epoch: 54\t w: 2.06\t b: 0.78\t L: 0.1913\n",
            "Epoch: 55\t w: 2.06\t b: 0.78\t L: 0.1874\n",
            "Epoch: 56\t w: 2.06\t b: 0.79\t L: 0.1835\n",
            "Epoch: 57\t w: 2.06\t b: 0.79\t L: 0.1797\n",
            "Epoch: 58\t w: 2.06\t b: 0.80\t L: 0.1760\n",
            "Epoch: 59\t w: 2.05\t b: 0.80\t L: 0.1724\n",
            "Epoch: 60\t w: 2.05\t b: 0.80\t L: 0.1689\n",
            "Epoch: 61\t w: 2.05\t b: 0.81\t L: 0.1655\n",
            "Epoch: 62\t w: 2.05\t b: 0.81\t L: 0.1621\n",
            "Epoch: 63\t w: 2.05\t b: 0.81\t L: 0.1589\n",
            "Epoch: 64\t w: 2.05\t b: 0.82\t L: 0.1557\n",
            "Epoch: 65\t w: 2.05\t b: 0.82\t L: 0.1526\n",
            "Epoch: 66\t w: 2.05\t b: 0.83\t L: 0.1496\n",
            "Epoch: 67\t w: 2.05\t b: 0.83\t L: 0.1466\n",
            "Epoch: 68\t w: 2.04\t b: 0.83\t L: 0.1437\n",
            "Epoch: 69\t w: 2.04\t b: 0.84\t L: 0.1409\n",
            "Epoch: 70\t w: 2.04\t b: 0.84\t L: 0.1382\n",
            "Epoch: 71\t w: 2.04\t b: 0.84\t L: 0.1355\n",
            "Epoch: 72\t w: 2.04\t b: 0.85\t L: 0.1329\n",
            "Epoch: 73\t w: 2.04\t b: 0.85\t L: 0.1304\n",
            "Epoch: 74\t w: 2.04\t b: 0.85\t L: 0.1279\n",
            "Epoch: 75\t w: 2.04\t b: 0.86\t L: 0.1255\n",
            "Epoch: 76\t w: 2.04\t b: 0.86\t L: 0.1231\n",
            "Epoch: 77\t w: 2.04\t b: 0.86\t L: 0.1208\n",
            "Epoch: 78\t w: 2.03\t b: 0.87\t L: 0.1185\n",
            "Epoch: 79\t w: 2.03\t b: 0.87\t L: 0.1164\n",
            "Epoch: 80\t w: 2.03\t b: 0.87\t L: 0.1142\n",
            "Epoch: 81\t w: 2.03\t b: 0.88\t L: 0.1121\n",
            "Epoch: 82\t w: 2.03\t b: 0.88\t L: 0.1101\n",
            "Epoch: 83\t w: 2.03\t b: 0.88\t L: 0.1081\n",
            "Epoch: 84\t w: 2.03\t b: 0.89\t L: 0.1062\n",
            "Epoch: 85\t w: 2.03\t b: 0.89\t L: 0.1043\n",
            "Epoch: 86\t w: 2.03\t b: 0.89\t L: 0.1024\n",
            "Epoch: 87\t w: 2.03\t b: 0.89\t L: 0.1006\n",
            "Epoch: 88\t w: 2.03\t b: 0.90\t L: 0.0989\n",
            "Epoch: 89\t w: 2.03\t b: 0.90\t L: 0.0972\n",
            "Epoch: 90\t w: 2.02\t b: 0.90\t L: 0.0955\n",
            "Epoch: 91\t w: 2.02\t b: 0.91\t L: 0.0939\n",
            "Epoch: 92\t w: 2.02\t b: 0.91\t L: 0.0923\n",
            "Epoch: 93\t w: 2.02\t b: 0.91\t L: 0.0907\n",
            "Epoch: 94\t w: 2.02\t b: 0.91\t L: 0.0892\n",
            "Epoch: 95\t w: 2.02\t b: 0.92\t L: 0.0877\n",
            "Epoch: 96\t w: 2.02\t b: 0.92\t L: 0.0863\n",
            "Epoch: 97\t w: 2.02\t b: 0.92\t L: 0.0849\n",
            "Epoch: 98\t w: 2.02\t b: 0.92\t L: 0.0835\n",
            "Epoch: 99\t w: 2.02\t b: 0.93\t L: 0.0822\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad3Q827dMohe"
      },
      "source": [
        "### Predictions\n",
        "\n",
        "Now that your model is trained, you can use it to predict some unknown test instances. Complete the code below to predict the output of the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTqEAz6GMoRH",
        "outputId": "df19cf35-2c47-4bcc-d454-ec513ff26e17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_predictions = np.zeros((len(y_test),))\n",
        "for (i, x) in enumerate(x_test):\n",
        "    # TODO: Complete this\n",
        "    y_predictions[i] = model.forward(x)\n",
        "\n",
        "print(y_predictions)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 5.9709949   6.98002215 10.0071039 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2ANPLrkLVaY"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Finally, let us evaluate the linear regression model you developed. Unlike classification, we will need a different metric for regression. Recall from Lecture 3, a common evaluation metric for regression is the Mean Squared Error (MSE). We will use that for this tutorial.\n",
        "\n",
        "Complete the `mse()` function below to compute the MSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fvD2vNXLx6f",
        "outputId": "44ecbc40-7622-42e7-b81d-91d3848dc130",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def mse(y_gold, y_prediction):\n",
        "    \"\"\" Compute the MSE given the ground truth and predictions\n",
        "\n",
        "    Args:\n",
        "        y_gold (np.ndarray): the correct ground truth values of y\n",
        "        y_prediction (np.ndarray): the predicted values of y\n",
        "\n",
        "    Returns:\n",
        "        float : MSE\n",
        "    \"\"\"\n",
        "\n",
        "    assert len(y_gold) == len(y_prediction)  \n",
        "    \n",
        "    # TODO: Complete this\n",
        "    return np.sum(np.square(y_prediction - y_gold)) / len(y_gold)\n",
        "\n",
        "\n",
        "# Compute the MSE on model predictions on our toy test data\n",
        "# You should be able to obtain a very small MSE rate\n",
        "print(mse(y_test, y_predictions))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0032900319172490095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6emtY9qqwGSe"
      },
      "source": [
        "## Iris Dataset (Extra exercise)\n",
        "\n",
        "Here is an extra optional exercise for you: try to get your simple linear regression model working on a (slightly) larger and noisier dataset. \n",
        "\n",
        "For this, we will convert the Iris dataset to use as a regression task. More specifically, our task is to predict the *petal width* of a flower (`y`) given the *sepal length* as input (`x`). The code below will give you the dataset in this format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1yTkyHzwnUp",
        "outputId": "4f4bce0a-2c6f-4074-ecf4-97f357e137d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "\n",
        "# Download iris data if it does not exist\n",
        "if not os.path.exists(\"iris.data\"):\n",
        "    !wget -O iris.data https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
        "\n",
        "def read_dataset_as_regression(filepath):\n",
        "    \"\"\" Read in the dataset from the specified filepath\n",
        "\n",
        "    Args:\n",
        "        filepath (str): The filepath to the dataset file\n",
        "\n",
        "    Returns:\n",
        "        tuple: returns a tuple of (x, y), each being a numpy array. \n",
        "               - x is a numpy array with shape (N, ), \n",
        "                   where N is the number of instances\n",
        "               - y is a numpy array with shape (N, ), where each element is a \n",
        "                real-valued float, and N is the number of instances\n",
        "    \"\"\"\n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "    for line in open(filepath):\n",
        "        if line.strip() != \"\": # handle empty rows in file\n",
        "            row = line.strip().split(\",\")\n",
        "            # extract columns 0 as x.\n",
        "            x.append(float(row[0])) \n",
        "\n",
        "            # extract column 3 as y\n",
        "            y.append(float(row[3]))\n",
        "\n",
        "    return (np.array(x), np.array(y))\n",
        "\n",
        "\n",
        "def split_dataset(x, y, test_proportion, random_generator=default_rng()):\n",
        "    \"\"\" Split dataset into training and test sets, according to the given \n",
        "        test set proportion.\n",
        "    \n",
        "    Args:\n",
        "        x (np.ndarray): Instances, numpy array with shape (N,K)\n",
        "        y (np.ndarray): Output label, numpy array with shape (N,)\n",
        "        test_proprotion (float): the desired proportion of test examples \n",
        "                                 (0.0-1.0)\n",
        "        random_generator (np.random.Generator): A random generator\n",
        "\n",
        "    Returns:\n",
        "        tuple: returns a tuple of (x_train, x_test, y_train, y_test) \n",
        "               - x_train (np.ndarray): Training instances shape (N_train, K)\n",
        "               - x_test (np.ndarray): Test instances shape (N_test, K)\n",
        "               - y_train (np.ndarray): Training labels, shape (N_train, )\n",
        "               - y_test (np.ndarray): Test labels, shape (N_test, )\n",
        "    \"\"\"\n",
        "\n",
        "    shuffled_indices = random_generator.permutation(len(x))\n",
        "    n_test = round(len(x) * test_proportion)\n",
        "    n_train = len(x) - n_test\n",
        "    x_train = x[shuffled_indices[:n_train]]\n",
        "    y_train = y[shuffled_indices[:n_train]]\n",
        "    x_test = x[shuffled_indices[n_train:]]\n",
        "    y_test = y[shuffled_indices[n_train:]]\n",
        "    return (x_train, x_test, y_train, y_test)\n",
        "\n",
        "\n",
        "(x, y) = read_dataset_as_regression(\"iris.data\")\n",
        "print(x.shape)  # (150,) \n",
        "print(y.shape)  # (150,)\n",
        "\n",
        "seed = 60012\n",
        "rg = default_rng(seed)\n",
        "x_train, x_test, y_train, y_test = split_dataset(x, y, \n",
        "                                                 test_proportion=0.2, \n",
        "                                                 random_generator=rg)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-04 11:03:04--  https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4551 (4.4K) [application/x-httpd-php]\n",
            "Saving to: iris.data\n",
            "\n",
            "\riris.data             0%[                    ]       0  --.-KB/s               \riris.data           100%[===================>]   4.44K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-11-04 11:03:04 (138 MB/s) - iris.data saved [4551/4551]\n",
            "\n",
            "(150,)\n",
            "(150,)\n",
            "(120,)\n",
            "(30,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XM2yXt_0RMO"
      },
      "source": [
        "As usual, it's always a good idea to examine your data before you start:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTF3kSAV0ddn",
        "outputId": "6e53e338-3ff5-4ede-ed47-e0ea403a81fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "plt.figure()\n",
        "plt.scatter(x_train, y_train, c=\"blue\", edgecolor='k')\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYm-2dB41JYI"
      },
      "source": [
        "### Model\n",
        "\n",
        "You can copy your `SimpleLinearRegression` class from earlier, with the`forward()`, `loss()` and `gradient()` methods that you implemented earlier. This time you can put them all together in the same place."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfdsAHaV1IuR"
      },
      "source": [
        "class SimpleLinearRegression:\n",
        "    def __init__(self, random_generator=default_rng()):\n",
        "        self.w = random_generator.standard_normal()\n",
        "        self.b = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w * x + self.b\n",
        "\n",
        "    def loss(self, x, y):\n",
        "        return np.square(self.forward(x) - y)\n",
        "\n",
        "    def gradient(self, x, y):\n",
        "        y_hat = self.forward(x)\n",
        "        return ((y_hat - y) * x, (y_hat - y))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6zXgIed3gTR"
      },
      "source": [
        "### Optimisation\n",
        "\n",
        "Again, you should not need to change much of your gradient descent implementation from earlier, so just copy it over. You may, however, need to tweak the learning rate and the number of epochs to obtain a reasonable output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Txe_vse3iIH",
        "outputId": "efa30ad3-711c-4df4-8a3c-aefe8557827e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = SimpleLinearRegression()\n",
        "\n",
        "learning_rate = 0.0001\n",
        "n_epochs = 100\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    error = 0.0\n",
        "    grad_w = 0.0\n",
        "    grad_b = 0.0\n",
        "    for (x, y) in zip(x_train, y_train):\n",
        "        ### TODO: Complete this\n",
        "        ### 1. Compute the gradients for w and b for this example\n",
        "        dLdw, dLdb = model.gradient(x, y)\n",
        "\n",
        "        ### 2. Add the gradients to grad_w and grad_b\n",
        "        grad_w += dLdw\n",
        "        grad_b += dLdb\n",
        "\n",
        "        ### 3. Add the \"local\" loss to the global error (Loss) for analysis\n",
        "        error += model.loss(x, y)\n",
        "\n",
        "    # TODO: Update the weights using the (summed) gradients\n",
        "    model.w = model.w - (learning_rate * grad_w)\n",
        "    model.b = model.b - (learning_rate * grad_b)\n",
        "    \n",
        "    print(f\"Epoch: {epoch}\\t w: {model.w:.2f}\\t b: {model.b:.2f}\\t L: {error:.4f}\")\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\t w: -0.29\t b: 0.06\t L: 3295.8685\n",
            "Epoch: 1\t w: -0.08\t b: 0.10\t L: 1098.8443\n",
            "Epoch: 2\t w: 0.04\t b: 0.11\t L: 388.1994\n",
            "Epoch: 3\t w: 0.10\t b: 0.13\t L: 158.3306\n",
            "Epoch: 4\t w: 0.14\t b: 0.13\t L: 83.9710\n",
            "Epoch: 5\t w: 0.16\t b: 0.13\t L: 59.9116\n",
            "Epoch: 6\t w: 0.18\t b: 0.14\t L: 52.1220\n",
            "Epoch: 7\t w: 0.18\t b: 0.14\t L: 49.5949\n",
            "Epoch: 8\t w: 0.19\t b: 0.14\t L: 48.7700\n",
            "Epoch: 9\t w: 0.19\t b: 0.14\t L: 48.4958\n",
            "Epoch: 10\t w: 0.19\t b: 0.14\t L: 48.3996\n",
            "Epoch: 11\t w: 0.19\t b: 0.13\t L: 48.3610\n",
            "Epoch: 12\t w: 0.19\t b: 0.13\t L: 48.3411\n",
            "Epoch: 13\t w: 0.19\t b: 0.13\t L: 48.3271\n",
            "Epoch: 14\t w: 0.19\t b: 0.13\t L: 48.3152\n",
            "Epoch: 15\t w: 0.19\t b: 0.13\t L: 48.3039\n",
            "Epoch: 16\t w: 0.19\t b: 0.13\t L: 48.2928\n",
            "Epoch: 17\t w: 0.19\t b: 0.13\t L: 48.2817\n",
            "Epoch: 18\t w: 0.19\t b: 0.13\t L: 48.2707\n",
            "Epoch: 19\t w: 0.19\t b: 0.13\t L: 48.2597\n",
            "Epoch: 20\t w: 0.19\t b: 0.13\t L: 48.2487\n",
            "Epoch: 21\t w: 0.19\t b: 0.13\t L: 48.2377\n",
            "Epoch: 22\t w: 0.20\t b: 0.13\t L: 48.2267\n",
            "Epoch: 23\t w: 0.20\t b: 0.13\t L: 48.2158\n",
            "Epoch: 24\t w: 0.20\t b: 0.13\t L: 48.2048\n",
            "Epoch: 25\t w: 0.20\t b: 0.12\t L: 48.1938\n",
            "Epoch: 26\t w: 0.20\t b: 0.12\t L: 48.1828\n",
            "Epoch: 27\t w: 0.20\t b: 0.12\t L: 48.1719\n",
            "Epoch: 28\t w: 0.20\t b: 0.12\t L: 48.1609\n",
            "Epoch: 29\t w: 0.20\t b: 0.12\t L: 48.1500\n",
            "Epoch: 30\t w: 0.20\t b: 0.12\t L: 48.1390\n",
            "Epoch: 31\t w: 0.20\t b: 0.12\t L: 48.1281\n",
            "Epoch: 32\t w: 0.20\t b: 0.12\t L: 48.1171\n",
            "Epoch: 33\t w: 0.20\t b: 0.12\t L: 48.1062\n",
            "Epoch: 34\t w: 0.20\t b: 0.12\t L: 48.0953\n",
            "Epoch: 35\t w: 0.20\t b: 0.12\t L: 48.0843\n",
            "Epoch: 36\t w: 0.20\t b: 0.12\t L: 48.0734\n",
            "Epoch: 37\t w: 0.20\t b: 0.12\t L: 48.0625\n",
            "Epoch: 38\t w: 0.20\t b: 0.11\t L: 48.0516\n",
            "Epoch: 39\t w: 0.20\t b: 0.11\t L: 48.0407\n",
            "Epoch: 40\t w: 0.20\t b: 0.11\t L: 48.0298\n",
            "Epoch: 41\t w: 0.20\t b: 0.11\t L: 48.0189\n",
            "Epoch: 42\t w: 0.20\t b: 0.11\t L: 48.0080\n",
            "Epoch: 43\t w: 0.20\t b: 0.11\t L: 47.9971\n",
            "Epoch: 44\t w: 0.20\t b: 0.11\t L: 47.9862\n",
            "Epoch: 45\t w: 0.20\t b: 0.11\t L: 47.9754\n",
            "Epoch: 46\t w: 0.20\t b: 0.11\t L: 47.9645\n",
            "Epoch: 47\t w: 0.20\t b: 0.11\t L: 47.9536\n",
            "Epoch: 48\t w: 0.20\t b: 0.11\t L: 47.9427\n",
            "Epoch: 49\t w: 0.20\t b: 0.11\t L: 47.9319\n",
            "Epoch: 50\t w: 0.20\t b: 0.11\t L: 47.9210\n",
            "Epoch: 51\t w: 0.20\t b: 0.11\t L: 47.9102\n",
            "Epoch: 52\t w: 0.20\t b: 0.10\t L: 47.8993\n",
            "Epoch: 53\t w: 0.20\t b: 0.10\t L: 47.8885\n",
            "Epoch: 54\t w: 0.20\t b: 0.10\t L: 47.8777\n",
            "Epoch: 55\t w: 0.20\t b: 0.10\t L: 47.8668\n",
            "Epoch: 56\t w: 0.20\t b: 0.10\t L: 47.8560\n",
            "Epoch: 57\t w: 0.20\t b: 0.10\t L: 47.8452\n",
            "Epoch: 58\t w: 0.20\t b: 0.10\t L: 47.8344\n",
            "Epoch: 59\t w: 0.20\t b: 0.10\t L: 47.8235\n",
            "Epoch: 60\t w: 0.20\t b: 0.10\t L: 47.8127\n",
            "Epoch: 61\t w: 0.20\t b: 0.10\t L: 47.8019\n",
            "Epoch: 62\t w: 0.20\t b: 0.10\t L: 47.7911\n",
            "Epoch: 63\t w: 0.20\t b: 0.10\t L: 47.7803\n",
            "Epoch: 64\t w: 0.20\t b: 0.10\t L: 47.7695\n",
            "Epoch: 65\t w: 0.20\t b: 0.10\t L: 47.7588\n",
            "Epoch: 66\t w: 0.20\t b: 0.09\t L: 47.7480\n",
            "Epoch: 67\t w: 0.20\t b: 0.09\t L: 47.7372\n",
            "Epoch: 68\t w: 0.20\t b: 0.09\t L: 47.7264\n",
            "Epoch: 69\t w: 0.20\t b: 0.09\t L: 47.7157\n",
            "Epoch: 70\t w: 0.20\t b: 0.09\t L: 47.7049\n",
            "Epoch: 71\t w: 0.20\t b: 0.09\t L: 47.6941\n",
            "Epoch: 72\t w: 0.20\t b: 0.09\t L: 47.6834\n",
            "Epoch: 73\t w: 0.20\t b: 0.09\t L: 47.6726\n",
            "Epoch: 74\t w: 0.20\t b: 0.09\t L: 47.6619\n",
            "Epoch: 75\t w: 0.20\t b: 0.09\t L: 47.6511\n",
            "Epoch: 76\t w: 0.20\t b: 0.09\t L: 47.6404\n",
            "Epoch: 77\t w: 0.20\t b: 0.09\t L: 47.6297\n",
            "Epoch: 78\t w: 0.20\t b: 0.09\t L: 47.6190\n",
            "Epoch: 79\t w: 0.20\t b: 0.09\t L: 47.6082\n",
            "Epoch: 80\t w: 0.20\t b: 0.08\t L: 47.5975\n",
            "Epoch: 81\t w: 0.20\t b: 0.08\t L: 47.5868\n",
            "Epoch: 82\t w: 0.20\t b: 0.08\t L: 47.5761\n",
            "Epoch: 83\t w: 0.20\t b: 0.08\t L: 47.5654\n",
            "Epoch: 84\t w: 0.20\t b: 0.08\t L: 47.5547\n",
            "Epoch: 85\t w: 0.20\t b: 0.08\t L: 47.5440\n",
            "Epoch: 86\t w: 0.20\t b: 0.08\t L: 47.5333\n",
            "Epoch: 87\t w: 0.20\t b: 0.08\t L: 47.5226\n",
            "Epoch: 88\t w: 0.20\t b: 0.08\t L: 47.5119\n",
            "Epoch: 89\t w: 0.20\t b: 0.08\t L: 47.5013\n",
            "Epoch: 90\t w: 0.20\t b: 0.08\t L: 47.4906\n",
            "Epoch: 91\t w: 0.20\t b: 0.08\t L: 47.4799\n",
            "Epoch: 92\t w: 0.20\t b: 0.08\t L: 47.4693\n",
            "Epoch: 93\t w: 0.20\t b: 0.08\t L: 47.4586\n",
            "Epoch: 94\t w: 0.20\t b: 0.07\t L: 47.4479\n",
            "Epoch: 95\t w: 0.20\t b: 0.07\t L: 47.4373\n",
            "Epoch: 96\t w: 0.20\t b: 0.07\t L: 47.4267\n",
            "Epoch: 97\t w: 0.20\t b: 0.07\t L: 47.4160\n",
            "Epoch: 98\t w: 0.20\t b: 0.07\t L: 47.4054\n",
            "Epoch: 99\t w: 0.20\t b: 0.07\t L: 47.3947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P7tQX-L31LB"
      },
      "source": [
        "### Visualising your trained model\n",
        "\n",
        "You can visualise your model by plotting the line on the graph.\n",
        "\n",
        "We will also plot the test instances to get a rough idea of how well we expect it to perform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUmFEg7s33Ve",
        "outputId": "6f7f2e5b-9833-4547-ce5b-f1d6f36e644d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "# Plot training instances\n",
        "plt.figure()\n",
        "plt.scatter(x_train, y_train, c=\"blue\", edgecolor='k')\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "\n",
        "# Draw the line representing the model\n",
        "xmin = x_train.min()\n",
        "ymin = model.forward(xmin)\n",
        "xmax = x_train.max()\n",
        "ymax = model.forward(xmax)\n",
        "plt.plot([xmin, xmax], [ymin, ymax], 'r-')\n",
        "\n",
        "# Plot test instances\n",
        "plt.scatter(x_test, y_test, c=\"red\", edgecolor='k')\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY37EHKe5JZT"
      },
      "source": [
        "### Predictions and evaluation\n",
        "\n",
        "Finally, predict the test instances given the model.\n",
        "\n",
        "Then evaluate the model with MSE.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHx0gaAr5OfN",
        "outputId": "d67bd42c-77e5-42ca-a1b4-219463da0105",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_predictions = np.zeros((len(y_test),))\n",
        "for (i, x) in enumerate(x_test):\n",
        "    y_predictions[i] = model.forward(x)\n",
        "\n",
        "print(y_predictions)\n",
        "print(y_test)\n",
        "\n",
        "print(mse(y_test, y_predictions))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.41982201 1.3585046  1.21543064 1.68553079 1.11323495 1.1949915\n",
            " 1.58333511 1.11323495 1.05191754 1.39938287 1.46070028 1.25630891\n",
            " 1.33806546 1.13367409 1.33806546 1.07235668 0.970161   1.48113942\n",
            " 1.31762632 1.44026115 1.21543064 1.31762632 1.31762632 1.09279582\n",
            " 1.50157856 0.94972186 1.05191754 1.17455237 1.03147841 0.99060013]\n",
            "[1.4 1.8 1.1 2.  0.2 1.1 1.9 0.4 0.1 1.8 2.3 2.4 1.8 0.2 2.3 0.1 0.2 2.3\n",
            " 1.4 1.7 2.  1.2 1.4 1.  1.4 0.1 0.3 0.4 0.2 0.3]\n",
            "0.4281067579973498\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNMajcyH7Vv5"
      },
      "source": [
        "## Summary\n",
        "\n",
        "Hopefully you have managed to deepen your understanding about linear regression by implementing the model, loss function, and the gradient descent algorithm, and putting everything together for training and testing.\n",
        "\n",
        "In the next lab exercise, we will delve a bit deeper at implementation level, and try to extend your model to handle more than one input variable. We will also start making your code a bit more efficient with vectorised implementations so that you can perform computations on multiple training instances simultaneously. This will hopefully help you get started on implementing Neural Networks (which we will unfortunately not cover in these lab exercises as it is part of your second coursework).  \n",
        "\n"
      ]
    }
  ]
}